{
    "# Paper-translation 论文翻译小助手\n\t\nAutomatic translation of Chinese and English papers\n\n## requirement\nBased on PyQt5, python3\n\ninstall by pip just like:\n```bash\npip install clipboard pyqt5\n```\n\n## how to run\nrun:\n```bash\npython paperTran.py\n```\n": "#纸张-基于PyQt 5，python 3由pip安装的汉英论文#要求的自动翻译，如：``bash pip安装剪贴板pyqt 5```#如何运行：``bash python纸质传输```bash pip安装剪贴板pyqt 5```",
    "Information networks are ubiquitous in the real world with\nexamples such as airline networks, publication networks, so-\ncial and communication networks, and the World Wide Web.\nThe size of these information networks ranges from hundreds\nof nodes to millions and billions of nodes. Analyzing large\ninformation networks has been attracting increasing atten-\ntion in both academia and industry. This paper studies\nthe problem of embedding information networks into low-\ndimensional spaces, in which every vertex is represented as\na low-dimensional vector. Such a low-dimensional embed-\nding is very useful in a variety of applications such as vi-\nsualization [21], node classification [3], link prediction [10],\nand recommendation [23].": "信息网络在现实世界中无处不在，例如航空公司网络、出版网络、社会和通信网络以及万维网。这些信息网络的规模从数百个节点到数百万到数十亿节点不等。分析大型信息网络已经引起学术界和工业界越来越多的关注。本文研究了在低维空间中嵌入信息网络的问题，其中每个顶点都表示为一个低维向量。这种低维嵌入在可视化[21]、节点分类[3]、链接预测[10]和建议[23]等各种应用中都非常有用。",
    "# Paper-translation 论文翻译小助手\n\t\nAutomatic translation of Chinese and English papers\n自动翻译中英文论文\n复制即可翻译\n\n## requirement\nBased on PyQt5, python3\n\ninstall by pip just like:\n```bash\npip install clipboard pyqt5\n```\n\n## how to run\nrun:\n```bash\npython paperTran.py\n```\n\n## features\n- detect clipboard\n- auto replace ‘\\n’\n![Alt text](./1541489044606.png)\n\n- 检测剪切板\n- 自动替换论文中的换行符": "# Paper-translation paper translation assistant Automatic translation of Chinese and English papers automatic translation English paper copy can translate # requirement Based on PyQt5, Python3 install by pip just like: `` bash pip install clipboard pyqt5'# how to run run:'`` bash python paperTran.py `` `# features detect clipboard auto replace'\\ n'! [Alt text] (. / 1541489044606.png) detects the newline characters in the paper for automatic replacement of shear plates",
    "/home/liwenjie/liwenjie/projects/CiBa-translation/utils/Paper-translation 论文翻译小助手/1541489044606.png": "/ home/liwenjie/projects/CiBa-translation/utils/Paper-translation thesis Translation Assistant / 1541489044606.png",
    "因为自己在看论文，同时需要翻译，但是论文里面的换行实在是太蛋疼了，于是写了个小工具帮助自己提高看论文的效率，各位老哥需要的也可以用用\nhttps://github.com/lkjie/paper-translation\n复制了论文到剪切板就能出结果": "Because I was reading a paper and needed to be translated, but the change in the line in the paper was really too painful, so I wrote a little tool to help myself improve the efficiency of reading the paper. You guys can also use https://github.com/lkjie/paper-translation to copy the paper to the shear plate and get the results.",
    "The second-\norder proximity between a pair of vertices (u, v) in a net-\nwork is the similarity between their neighborhood network\nstructures. Mathematically, let p u = (w u,1 , . . . , w u,|V | ) de-\nnote the first-order proximity of u with all the other vertices,\nthen the second-order proximity between u and v is deter-\nmined by the similarity between p u and p v . If no vertex is\nlinked from/to both u and v, the second-order proximity\nbetween u and v is 0.": "网络中一对顶点(u，v)之间的二阶邻近性是它们的邻域网络结构之间的相似性。从数学上讲，设pu=(wu，1，。。。表示u与所有其它顶点的一阶邻近度，然后由pu与pv之间的相似性来确定u与v之间的二阶接近。如果没有从u和v连接的顶点，则u和v之间的二阶邻近度为0。",
    "[图片上传失败...(image-b56dbc-1541489838700)]\n": "[image upload failed. (image-b56dbc-1541489838700)]",
    "因为自己在看论文，同时需要翻译，但是论文里面的换行实在是太蛋疼了，于是写了个小工具帮助自己提高看论文的效率，各位老哥需要的也可以用用\nhttps://github.com/lkjie/paper-translation\n复制了论文到剪切板就能出结果\n": "Because I was reading a paper and needed to be translated, but the change in the line in the paper was really too painful, so I wrote a little tool to help myself improve the efficiency of reading the paper. You guys can also use https://github.com/lkjie/paper-translation to copy the paper to the shear plate and get the results.",
    "自律指数 消费数据、宿舍门禁数据\n经济指数 消费数据\n阅读指数 图书借阅数据、图书馆门禁数据\n学霸指数 图书借阅数据、成绩数据、四六级\n卓越指数 创新创业数据、竞赛获奖数据、社会实践数据、论文、软著\n": "Self-discipline index consumption data, dormitory entrance control data economy index consumption data, reading index books borrowing data, library entrance control data high achiever index books borrowing data, achievement data, Class 4 and 6 Excellence Indices Innovation and Entrepreneurship data, contest award-winning data, Social practice data, papers, soft works",
    "：": ":",
    "# Paper-translation 论文翻译小助手\n\t\nAutomatic translation of Chinese and English papers\n自动翻译中英文论文\n复制即可翻译\n\n## requirement\nBased on PyQt5, python3\n\ninstall by pip just like:\n```bash\npip install clipboard pyqt5\n```\n\n需要PyQt5, python3的支持，利用pip安装即可\n\n## how to run\nrun:\n```bash\npython paperTran.py\n```\n\n首先需要安装python，推荐anaconda\n\n## features\n- detect clipboard\n- auto replace ‘\\n’\n\n![Alt text](./1541489044606.png)\n\n- 检测剪切板\n- 自动替换论文中的换行符\n\n## ": "# Paper-translation paper translation assistant Automatic translation of Chinese and English papers automatic translation of English and Chinese papers can translate # requirement Based on PyQt5, python3 install by pip just like: ```````bash pip install clipboard pyqt5 `require the support of PyQt5, python3, # how to run run: '````bash python paperTran.py '`can be installed using pip first of all you need to install python, Anaconda # features detect clipboard auto replace'\\ n'! [Alt text] (. / 1541489044606.png) is recommended to detect automatic replacement of line breaks in the clipboard",
    "https://www.anaconda.com/download/": "https://www.anaconda.com/download",
    "首先需要安装python，推荐[anaconda][https://www.anaconda.com/download/]\n": "First you need to install the python, recommendation [anaconda] [https://www.anaconda.com/download/]]",
    "# Paper-translation 论文翻译小助手\n\t\nAutomatic translation of Chinese and English papers\n自动翻译中英文论文\n复制即可翻译\n\n## requirement\nBased on PyQt5, python3\n\ninstall by pip just like:\n```bash\npip install clipboard pyqt5\n```\n\n需要PyQt5, python3的支持，利用pip安装即可\n\n## how to run\nrun:\n```bash\npython paperTran.py\n```\n\n首先需要安装python，推荐[anaconda][https://www.anaconda.com/download/]\n\n## features\n- detect clipboard\n- auto replace ‘\\n’\n\n![Alt text](./1541489044606.png)\n\n- 检测剪切板\n- 自动替换论文中的换行符\n\n## ": "# Paper-translation paper translation assistant Automatic translation of Chinese and English papers automatic translation of English and Chinese papers can translate # requirement Based on PyQt5, python3 install by pip just like: ```````bash pip install clipboard pyqt5 `require the support of PyQt5, python3, # how to run run: '````bash python paperTran.py '`can be installed using pip first of all you need to install python, Recommends [anaconda] [https://www.anaconda.com/download/] # features detect clipboard auto replace'\\ n'! [Alt text] (. / 1541489044606.png) to detect line breaks in automatic replacement of shear plates #",
    "[https://www.anaconda.com/download/]": "[https://www.anaconda.com/download/]",
    "奖助贷数据\r消费数据\r成绩数据\r图书馆数据\r门禁数据\r实习科研数据\r": "Data consumption data achievements data access data practice data research data in data library",
    "实习科研数据": "Practice scientific research data",
    "file:///home%2Fliwenjie%2F.cxoffice%2Fapps.com.qq.im%2Fdosdevices%2Fc%3A%2Fusers%2Fcrossover%2FMy Documents%2FTencent Files%2F348808705%2FImage%2FC2C%2F_RRLM%5BJ3%40UT9EZ8%24%24YZKD%249.png": "文件：/HOME/liwenjie/.cxoffice/apps.com.qq.im/dostions/c：/user/跨界/My Documents/腾讯Files/348808705/Image/C2C/_RLM[J3@UT9EZ8$9.png]",
    "file:///home%2Fliwenjie%2F.cxoffice%2Fapps.com.qq.im%2Fdosdevices%2Fc%3A%2Fusers%2Fcrossover%2FMy Documents%2FTencent Files%2F348808705%2FImage%2FC2C%2F%60%7D%258%29AST%5BAQ8A%5BX%60ZFQG%7D~8.jpg": "文件：/home/liwenjie/.cxoffice/apps.com.qq.im/dostions/c：/user/Cross/my Documents/腾讯Files/348808705/Image/C2C/`}%8)AST[AQ8A[X`ZFQG}~8.jpg]",
    "file:///home%2Fliwenjie%2F.cxoffice%2Fapps.com.qq.im%2Fdosdevices%2Fc%3A%2Fusers%2Fcrossover%2FMy Documents%2FTencent Files%2F348808705%2FImage%2FC2C%2FA7WI_F%60%7D1MK6K%7BZ_9%7DI%5D4F3.jpg": "文件：/HOME/liwenje/.cxoffice/apps.com.qq.im/投样器/c：/user/跨界/我的文档/腾讯文件/348808705/Image/C2C/A7WI_F`}1MK6K{Z_9}i]4F3.jpg",
    "/home/liwenjie/11.7讲ppt.pptx": "/ home/liwenjie/11.7 ppt.pptx",
    "已经改好了": "It's already fixed.",
    "差不多了": "Almost",
    "换卡\r\n序列1\r\n淋浴\r\n": "Card exchange sequence 1 showe",
    "```bash\npython paperTran.py\n```": "``bash python造纸业```‘",
    "```bash\npip install clipboard pyqt5\n```": "``bash pip安装剪贴板",
    "```bash\npython paperTran.py\n```\n": "``bash python造纸业```‘",
    "run:": "brun[人名] 布龙",
    "1. 首先需要安装python，推荐[anaconda][1]，因为PyQt5需要python3的支持，请选择python3的anaconda\n2. 安装完python后，利用python自带的包管理工具pip安装所需依赖\n```bash\npip install clipboard pyqt5\n```\n3. 完成后运行目录下的py脚本即可\n```bash\npython paperTran.py\n```": "1. First, you need to install the python, recommendation [anaconda] [1] because PyQt5 requires python3 support, select python3's anaconda 2. 2. After installing python, use python's own package management tool, pip, to install dependent on `` `bash pip install clipboard pyqt5'3. Once finished, run the py script in the directory to be able to '```bash python paperTran.py '`",
    "\n\n需要PyQt5, python3的支持，利用pip安装即可": "You need PyQt5, python3 support, and you can install it with pip",
    "\n首先需要安装python，推荐[anaconda][https://www.anaconda.com/download/]\n": "First you need to install the python, recommendation [anaconda] [https://www.anaconda.com/download/]]",
    "- 检测剪切板\n- 自动替换论文中的换行符\n": "Detect automatic replacement of new line characters in paper with shear plate",
    "\n## ": "##",
    "![Alt text](./1541489044606.png)": "[备选案文](.1541489044606.png)",
    "# Paper-translation 论文翻译小助手\n\t\nAutomatic translation of Chinese and English papers\n自动翻译中英文论文\n复制即可翻译\n\n## 使用说明\n\n1. 首先需要安装python，推荐[anaconda][1]，因为PyQt5需要python3的支持，请选择python3的anaconda\n2. 安装完python后，利用python自带的包管理工具pip安装所需依赖\n```bash\npip install clipboard pyqt5\n```\n3. 完成后运行目录下的py脚本即可\n```bash\npython paperTran.py\n```\n\n## 功能\n- 检测剪切板，实时翻译\n- 自动替换论文中的换行符\n- 对英文选择后，右下小窗口会出现选择英文的解释\n\n## requirement\nBased on PyQt5, python3\n\ninstall by pip just like:\n```bash\npip install clipboard pyqt5\n```\n\n## how to run\nrun:\n```bash\npython paperTran.py\n```\n\n## features\n- detect clipboard\n- auto replace ‘\\n’\n\n": "Paper-translation paper translation assistant Automatic translation of Chinese and English papers automatic translation English paper copy can translate # use description 1. First, you need to install the python, recommendation [anaconda] [1] because PyQt5 requires python3 support, select python3's anaconda 2. 2. After installing python, use python's own package management tool, pip, to install dependent on `` `bash pip install clipboard pyqt5'3. After completion, run the py script in the directory to detect the clipping board of'`` bash python paperTran.py'# function. In the bottom right window, the explanation # requirement Based on PyQt5, python3 install by pip just like: `` bash pip install clipboard pyqt5'# how to run run: `` `bash python paperTran.py` `# features detect clipboard auto replace'\\ n' appears in English.",
    "The LINE is very general, which works well for directed\nor undirected, weighted or unweighted graphs. We evalu-\nate the performance of the LINE with various real-world\ninformation networks, including language networks, social\nnetworks, and citation networks. The effectiveness of the\nlearned embeddings is evaluated within multiple data min-\ning tasks, including word analogy, text classification, and\nnode classification. The results suggest that the LINE model\noutperforms other competitive baselines in terms of both ef-\nfectiveness and efficiency. It is able to learn the embedding\nof a network with millions of nodes and billions of edges in\na few hours on a single machine.": "这条线是非常通用的，它适用于有向或无向、加权或非加权图。我们用各种真实世界的信息网络，包括语言网络、社交网络和引文网络来评估这条线路的性能。在多个数据挖掘任务(包括单词类推、文本分类和节点分类)中评估所学习的嵌入的有效性。结果表明，该线性模型在有效性和效率方面均优于其他竞争基线。它能够在一台机器上几个小时内学习数百万个节点和数十亿个边的网络的嵌入。",
    "analogy": "n. 类似，相似;比拟，类比;类推;",
    "/home/liwenjie/Documents/Paper-translation 论文翻译小助手.zip": "/ home/liwenjie/Documents/Paper-translation paper translation assistant. Zip",
    "/home/liwenjie/liwenjie/projects/paper-translation/doc/Paper-translation 论文翻译小助手.md": "/ home/liwenjie/projects/paper-translation/doc/Paper-translation paper translation assistant. Md",
    "/home/liwenjie/liwenjie/projects/paper-translation/Paper-translation 论文翻译小助手.zip": "/ home/liwenjie/projects/paper-translation/Paper-translation paper translation assistant. Zip",
    "README.md": "README.md",
    "- 检测剪切板，实时翻译\n- 自动替换论文中的换行符\n- 对英文选择后，右下小窗口会出现选择英文的解释\n![Alt text](./doc/1541495491986.png)\n- 翻译的句子会以json格式保存在当前目录下translated.json文件中\n- 自动置顶": "After checking the clipping board and automatically replacing the line pairs in the paper in real time translation, An explanation for choosing English appears in the lower right window! [Alt text] (. / doc/1541495491986.png) translated sentences will be saved in json format in the translated.json file in the current directory to set the top automatically",
    "![Alt text](./doc/1541495491986.png)": "[备选案文](/doc/1541495491986.ng)",
    "Check the clipping board, real-time translate-automatically replace the newline characters in the paper-after the English selection, An explanation of the choice of English appears in the lower right window-the translated sentences are saved in json format in the translated.json file in the current directory-auto top": "检查剪贴板，实时翻译-自动替换文件中的换行符-在英文选择之后，右下角显示对英语选择的解释-翻译后的句子以json格式保存在当前目录中的translated.json文件中-autotop。",
    "explanation": "n. 解释;说明;辩解;（消除误会后）和解;",
    "after the English selection, An ": "在英语选集之后，",
    "translated sentences": "翻译句",
    "explain": "vt.& vi. 讲解，解释;\nvt. 说明…的原因，辩解;\nvi. 说明，解释，辩解;",
    "of": "prep. 关于;属于…的;由…制成;\naux. 助动词  [非标准用语、方言] =have [主用于虚拟语气];",
    "xx": "abbr. without securities or warrants 无权或无保障（代号）;",
    "linked": "adj. 连接的，显示连环遗传的;\nv. 连接( link的过去式和过去分词 );联系;相关联;说明（两件东西或两人之间）有联系（或关系）;",
    "self.lastTran = clipText": "Sel.lastTran=clipText",
    "textSelected": "be selected入选；中选",
    "self.lastTranWord = textSelected": "Self.lastTranword=textSelected",
    "\n        ": "",
    "self.lastTranWord": "自我-最后一遍",
    "proximity": "n. 接近，邻近;接近度，距离;亲近;",
    "self.checkBox.setText(_translate(\"PaperTran\", \"检测剪切板\"))": "Self.checkBox.setText (_ translate (\"PaperTran\", \"detect shear plate\")",
    "setText": "设置文本",
    "We investigate both first-order and second-order proxim-\nity for network embedding, which is defined as follows.": "我们研究了网络嵌入的一阶和二阶邻近性，定义如下.",
    "Given a large network G = (V, E), the problem\nof Large-scale Information Network Embedding aims\nto represent each vertex v ∈ V into a low-dimensional space\nR d , i.e., learning a function f G : V → R d , where d \u001c |V |.\nIn the space R d , both the first-order proximity and the\nsecond-order proximity between the vertices are preserved.": "在给定一个大型网络G=(V，E)的情况下，大规模信息网络嵌入问题的目的是将每个顶点v∈V表示为低维空间Rd，即学习函数fG：v→Rd，其中d_(？)在空间R_d中，保留了顶点之间的一阶邻近和二阶邻近。",
    "each": "adj. 每;各自的;\npron. 每个;各自;",
    "represent": "vt. 表现，象征;代表，代理;扮演;作为示范;\nvi. 代表;提出异议;",
    "A desirable embedding model for real world information\nnetworks must satisfy several requirements: first, it must\nbe able to preserve both the first-order proximity and the\nsecond-order proximity between the vertices; second, it must\nscale for very large networks, say millions of vertices and bil-\nlions of edges; third, it can deal with networks with arbitrary\ntypes of edges: directed, undirected and/or weighted. In this\nsection, we present a novel network embedding model called\nthe “LINE,” which satisfies all the three requirements.": "一个理想的真实世界信息网络嵌入模型必须满足几个要求：第一，它必须能够同时保持顶点之间的一阶接近和二阶接近；第二，它必须对非常大的网络，例如数百万顶点和数十亿个边进行缩放；第三，它可以处理任意类型的边：有向、无向和/或加权的网络。在这一部分中，我们提出了一个新的网络嵌入模型，称为“线”，它满足了这三个需求。",
    "We describe the LINE model to preserve the first-order\nproximity and second-order proximity separately, and then\nintroduce a simple way to combine the two proximity.": "我们描述了分别保持一阶邻近和二阶邻近的直线模型，然后介绍了一种简单的方法来将这两种接近结合起来。",
    "The first-order proximity refers to the local pairwise prox-\nimity between the vertices in the network. To model the": "一阶邻近是指网络中各顶点之间的局部成对接近.建模",
    "The first-order proximity refers to the local pairwise proximity between the vertices in the network. To model the first-order proximity, for each undirected edge (i, j), we de-\nfine the joint probability between vertex v i and v j as follows:": "一阶邻近是指网络中各顶点之间的局部成对接近.为了对每个无向边(i，j)进行一阶邻近建模，我们定义了顶点vi和vj之间的联合概率如下：",
    "The second-order proximity is applicable for both directed\nand undirected graphs. Given a network, without loss of\ngenerality, we assume it is directed (an undirected edge can\nbe considered as two directed edges with opposite directions\nand equal weights). The second-order proximity assumes\nthat vertices sharing many connections to other vertices are\nsimilar to each other. In this case, each vertex is also treated\nas a specific “context” and vertices with similar distributions\nover the “contexts” are assumed to be similar. Therefore,\neach vertex plays two roles: the vertex itself and a specific\n“context” of other vertices. We introduce two vectors ~\nu i and\n~\nu 0 i , where ~\nu i is the representation of v i when it is treated\nas a vertex while ~\nu 0 i is the representation of v i when it is\ntreated as a specific “context”. For each directed edge (i, j),\nwe first define the probability of “context” v j generated by\nvertex v i as:": "二阶邻近度既适用于有向图，也适用于无向图.在不损失通用性的情况下，假设网络是有向的(无向边可以看作是两个方向相反、权重相等的有向边)。二阶邻近假设与其他顶点共享许多连接的顶点彼此相似。在这种情况下，每个顶点也被视为一个特定的“上下文”，并且假设在“上下文”上具有相似分布的顶点是相似的。因此，每个顶点扮演两个角色：顶点本身和其他顶点的特定“上下文”。我们引入了两个向量ui和u0i，其中u i是v i的表示，当它被当作顶点处理时，u0 i是v i的表示，当它被当作一个特定的“上下文”时，它是v i的表示。对于每个有向边(i，j)，我们首先将顶点vi生成的“上下文”vj的概率定义为：",
    "学生大数据预测分享\r": "Student big data forecast sharing",
    "/home/liwenjie/.cxoffice/apps.com.qq.im/dosdevices/c:/users/crossover/My Documents/Tencent Files/348808705/FileRecv/《PPT模板》.pptx": "/ home/liwenjie/.cxoffice/apps.com.qq.im/dosdevices/c:/users/crossover/My Documents/Tencent Files/348808705/FileRecv/ < PPT template >. Pptx",
    "553270571": "五亿五千三百二十七万零五百七十一",
    "ppt我来改，": "Ppt, I'll fix it.",
    "import os\nimport sys\nfrom preprocessing import read_df_from_mysql_db, list2file, file2list\nfrom dateutil.parser import parse\nimport pandas as pd\nimport numpy as np\nimport datetime as dt": "从导入预处理导入os导入sys-read_df_from_mysql_db、list2file、file2list-从dateutil.解析器导入解析导入熊猫作为pd导入numpy作为np导入日期时间为dt",
    "\nfrom preprocessing import read_df_from_mysql_db, list2file, file2list": "从预处理导入read_df_from_mysql_db，list2file，file2list",
    "Markdown Preview": "标价预览",
    "/media/liwenjie/rt/share/学生画像/images/2018/11/1.png": "/ media/liwenjie/rt/share/ student portrait / images/2018/11/1.png",
    "vscode markdow": "vscode标记",
    "xclip": "外部参照剪裁",
    "telesoho.MarkdownPaste": "射电",
    "/home/liwenjie/Documents/vscode": "/home/liwenjie/documents/vscode",
    "pdf": "abbr. point detonating fuse 弹头信管;弹头（引爆）引信;probability density function 概率密度函数;probability distribution function 概率分布函数;",
    "/home/liwenjie/Documents/vscode/Untitled.pdf": "/home/liwenjie/documents/vscode/unTitled.pdf",
    "Note that the first-order proximity is only applicable for\nundirected graphs, not for directed graphs. By finding the\n{~\nu i } i=1..|V | that minimize the objective in Eqn. (3), we can\nrepresent every vertex in the d-dimensional space.": "请注意，一阶邻近只适用于无向图，而不适用于有向图。通过寻找{u~i}i=1.x~(？)V，使Eqn中的目标最小化。(3)我们可以表示d维空间中的每个顶点。",
    "The second-order proximity is applicable for both directed and undirected graphs. Given a network, without loss of generality, we assume it is directed (an undirected edge can be considered as two directed edges with opposite directions and equal weights). The second-order proximity assumes that vertices sharing many connections to other vertices are similar to each other. In this case, each vertex is also treated as a specific “context” and vertices with similar distributions over the “contexts” are assumed to be similar. Therefore, each vertex plays two roles: the vertex itself and a specific “context” of other vertices. We introduce two vectors ~ u i and ~ u 0 i , where ~ u i is the representation of v i when it is treated as a vertex while ~ u 0 i is the representation of v i when it is treated as a specific “context”. For each directed edge (i, j), we first define the probability of “context” v j generated by vertex v i as:": "二阶邻近度既适用于有向图，也适用于无向图.在不损失通用性的情况下，假设网络是有向的(无向边可以看作是两个方向相反、权重相等的有向边)。二阶邻近假设与其他顶点共享许多连接的顶点彼此相似。在这种情况下，每个顶点也被视为一个特定的“上下文”，并且假设在“上下文”上具有相似分布的顶点是相似的。因此，每个顶点扮演两个角色：顶点本身和其他顶点的特定“上下文”。我们引入了两个向量ui和u0i，其中u i是v i的表示，当它被当作顶点处理时，u0 i是v i的表示，当它被当作一个特定的“上下文”时，它是v i的表示。对于每个有向边(i，j)，我们首先将顶点vi生成的“上下文”vj的概率定义为：",
    "where |V | is the number of vertices or “contexts.” For each\nvertex v i , Eqn. (4) actually defines a conditional distribution\np 2 (·|v i ) over the contexts, i.e., the entire set of vertices in the\nnetwork. As mentioned above, the second-order proximity\nassumes that vertices with similar distributions over the con-\ntexts are similar to each other. To preserve the second-order\nproximity, we should make the conditional distribution of\nthe contexts p 2 (·|v i ) specified by the low-dimensional rep-\nresentation be close to the empirical distribution p̂ 2 (·|v i ).\nTherefore, we minimize the following objective function:": "其中，维希是顶点或“上下文”的数目。对于每个顶点vi，Eqn。(4)在上下文中定义了一个条件分布p2(·vi)，即网络中的全部顶点集。如上所述，二阶邻近假设在上下文中具有相似分布的顶点彼此相似。为了保持二阶邻近性，我们应该使低维表示法所指定的上下文p2的条件分布接近于经验分布p̂2(·vi)。因此，我们将下列目标函数最小化：",
    "where |V | is the number of vertices or “contexts.” For each\nvertex v i , Eqn. (4) actually defines a conditional distribution\np 2 (·|v i ) over the contexts, i.e., the entire set of vertices in the\nnetwork. As mentioned above, the second-order proximity\nassumes that vertices with similar distributions over the con-\ntexts are similar to each other. To preserve the second-order\nproximity, we should make the conditional distribution of\nthe contexts p 2 (·|v i ) specified by the low-dimensional rep-\nresentation be close to the empirical distribution p̂ 2 (·|v i ).\nTherefore, we minimize the following objective function:\nX\nO 2 =\nλ i d(p̂ 2 (·|v i ), p 2 (·|v i )),\n(5)": "其中，维希是顶点或“上下文”的数目。对于每个顶点vi，Eqn。(4)在上下文中定义了一个条件分布p2(·vi)，即网络中的全部顶点集。如上所述，二阶邻近假设在上下文中具有相似分布的顶点彼此相似。为了保持二阶邻近性，我们应该使低维表示法所指定的上下文p2的条件分布接近于经验分布p̂2(·vi)。因此，我们将下列目标函数最小化：x O2=λi d(p̂2(·x？vi)，p2(·x？vi)，(5)",
    "that": "det. 那个，那;\npron. 那个，那;\nconj. 多么;如此…以至;用于某些动词、形容词和名词后，引出各种从句;\nadv. 不那么;那样;",
    "Orderliness predicts academic performance: behavioural analysis on campus lifestyle": "OrderLinessPrediCTSAcademicPerformance：BehaviouralAnalysison校区Liestyle",
    "Quantifying China’s regional economic complexity": "《中国区域经济综合指数》",
    "Impacts of opinion leaders on social contagions": "影响社会和传染性疾病的舆论的影响",
    "!&": "!&",
    "201622060228@std.uestc.edu": "201622060228@std.uestc.edu",
    "201622060228": "二千零一十六亿二千二百零六万零二百二十八",
    "Quantifying China’s Regional Economic Complexity": "量化中国区域经济复杂性",
    "ImpactsÂ ofÂ opinionÂ leadersÂ onÂ socialÂ contagions": "受影响的社会和社会的影响",
    "Link prediction via matrix completion\r\nRetractions: stamp out fake peer review\r\nEvaluating user reputation in online rating systems via an iterative groupbased ranking method\r\nScale-free networks without growth\r\nBounds of memory strength for power­law series": "基于矩阵完成收缩的链路预测：杜绝虚假的同行评审Evaluating user reputation in online rating systems via an iterative groupbased ranking method无增长Bounds of memory strength for power­law series无标度网络",
    "Link prediction via matrix completion": "基于矩阵完成的链路预测",
    "Retractions: stamp out fake peer review": "撤回：杜绝虚假的同行评审",
    "Link Prediction via Matrix Completion": "基于矩阵完成的链路预测",
    "Evaluating user reputation in online rating systems via an iterative groupbased ranking metho": "EvaluatingUserReputyinOnlineRatingSystemsviaanIerativeGroupBasedRankingMetho",
    "Evaluating user reputation in online rating systems via an iterative groupbased ranking method": "EvaluatingUserReputyinOnlineRatingSystemsviaaniterativeGroupBasedRankingMethod",
    "Scale-free networks without growth": "无规模无增长网络",
    "Bounds of memory strength for power­law serie": "BOUNDSofMemoryStrengthforPower-LawNo.",
    "Bounds of memory strength for power­law series": "BoundsofMemoryStrengthforPower-LawSeries",
    "Stamp out fake\npeer review": "杜绝虚假同行评审",
    "https://arxiv.org/pdf/1509.00594.pdf": "https://arxiv.org/pdf/1509.00594.pdf",
    "https://arxiv.org/pdf/1506.09096.pdf": "https://arxiv.org/pdf/1506.09096.pdf",
    "Evaluating user reputation in online rating systems via an i terative group-based ranking\nmethod": "通过一种基于迭代组的排序方法评估在线评级系统中的用户声誉",
    "Bounds of memory strength for power-law series": "幂律级数的记忆强度界",
    "/home/liwenjie/pdf.tar.gz": "/home/liwenje/pdf.tar.gz",
    "/home/liwenjie/Documents/论文/段老师/Scale-free networks without growth.pdf\n/home/liwenjie/Documents/论文/段老师/Retractions: stamp out fake peer review.pdf\n/home/liwenjie/Documents/论文/段老师/Link Prediction via Matrix Completion.pdf\n/home/liwenjie/Documents/论文/段老师/Quantifying China’s Regional Economic Complexity.pdf\n/home/liwenjie/Documents/论文/段老师/Orderliness predicts academic performance: behavioural analysis on campus lifestyle.pdf": "/ home/liwenjie/Documents/ Duan / Scale-free networks without growth.pdf / home/liwenjie/Documents/ Duan / Retractions: stamp out fake peer review.pdf / home/liwenjie/Documents/ Duan / Link Prediction via Matrix Completion.pdf / home/ Liwenjie/Documents/ paper / teacher Duan / Quantifying China's Regional Economic Complexity.pdf / home/liwenjie/Documents/ paper / teacher Duan / Orderliness predicts academic performance: behavioural analysis on campus lifestyle.pdf",
    "/home/liwenjie/Documents/论文/琪琪/pdf/Bounds of memory strength for power-law series.pdf\n/home/liwenjie/Documents/论文/琪琪/pdf/Evaluating user reputation in online rating systems via an i terative group-based ranking.pdf\n/home/liwenjie/Documents/论文/琪琪/pdf/Impacts of opinion leaders on social contagions.pdf": "/ home/liwenjie/Documents/ Gigi / pdf/Bounds of memory strength for power-law series.pdf / home/liwenjie/Documents/ Gigi / pdf/Evaluating user reputation in online rating systems via an i terative group-based ranking.pdf / home/liwenjie/Documents/ Gigi / pdf/Impacts of opinion leaders on social contagions.pdf",
    "arxiv": "预印本文献库",
    "prestige": "n. 威信，威望，声望;声誉;（财势的）显赫;信望;",
    "172.16.170.81": "172.16.170.81",
    "engine = create_engine('mysql+pymysql://%(user)s:%(pwd)s@%(host)s/%(dbname)s?charset=utf8' % {'user': utils.DB_OUT_USER, 'pwd': utils.DB_OUT_PWD, 'host': utils.DB_OUT_HOST, 'dbname': utils.DB_OUT_NAME},\n                                       encoding='utf-8')": "Engine=Create_Engine(‘mysql pymysql://%(user)s：%(pwd)s@%(host)s/%(dbname)s?charset=utf8’%{‘user’：utils.DB_out_user，‘pwd’：utils.DB_out_wd，‘host’：utils.DB_out_host，‘dbName’：utils.DB_out_name}，编码=‘utf-8’)",
    "import pandas as pd\nimport pymysql, logging, os\nimport sys, datetime\nfrom sqlalchemy import create_engine\nimport numpy as np\nfrom dateutil.relativedelta import relativedelta\nfrom scipy.stats import norm\nimport matplotlib.pyplot as plt\nimport queue, json, time, dateutil, math\nfrom collections import namedtuple\nimport json": "将熊猫作为pd导入pymysql、日志记录、os导入sys、sqlalchemy导入的日期时间创建_Engine导入numpy作为np从dateutil.相对论区导入相对论性导入从cip.stats导入规范导入matplotlib.pyart作为PLT导入队列、json、time、dateutil、数学从集合导入namedtuple导入json",
    "t_theme_index": "t主题索引",
    "DB_OUT_HOST = conf['DB_OUT_HOST']\nDB_OUT_PORT = conf['DB_OUT_PORT']\nDB_OUT_USER = conf['DB_OUT_USER']\nDB_OUT_PWD = conf['DB_OUT_PWD']\nDB_OUT_NAME = conf['DB_OUT_NAME']": "DB_OUT_HOST=CONF[‘DB_OUT_HOST’]DB_OUT_PORT=CONF[‘DB_OUT_USER’]DB_OUT_PWD=CONF[‘DB_OUT_PWD’]DB_OUT_USER=CONF[‘DB_OUT_PWD’]CONF[‘DB_OUT_NAME’]",
    "mysql+pymysql": "MySQL pymysql",
    "%(host)s": "%(host)s",
    "zhutao": "zhutao",
    "demo": "n. 演示;示威;样本唱片;民主党员;",
    "wpi_demo": "WPI演示",
    "#!/usr/bin/env python\n# -*- coding: utf-8 -*-\n\n__author__ = 'liwenjie'\n\nimport pandas as pd\nimport pymysql, logging, os\nimport sys, datetime\nfrom sqlalchemy import create_engine\nimport numpy as np\nfrom dateutil.relativedelta import relativedelta\nfrom scipy.stats import norm\nimport matplotlib.pyplot as plt\nimport queue, json, time, dateutil, math\nfrom collections import namedtuple\nimport json\n\n\nDB_OUT_HOST = '172.16.170.81'\nDB_OUT_PORT = 3308\nDB_OUT_USER = 'zhutao'\nDB_OUT_PWD = 'zhutao321'\nDB_OUT_NAME = 'wpi_demo'\n\n\nengine = create_engine('mysql+pymysql://%(user)s:%(pwd)s@%(host)s:%(port)s/%(dbname)s?charset=utf8' % {'user': DB_OUT_USER, 'pwd': DB_OUT_PWD, 'host': DB_OUT_HOST, 'dbname': DB_OUT_NAME, 'port':DB_OUT_PORT},\n                                       encoding='utf-8')\nconn = engine.connect()\n\ndf_theme = pd.read_sql_table('t_theme_index', conn)\n": "#！/usr/bin/env python#-*编码：utf-8-*_Author_=‘liwenje’导入熊猫作为PD导入pymysql，日志记录，os导入系统，从sqlalchemy导入创建_Engine导入numpy的日期时间，从utidatel.相对论性导入相对论性杰森，时间，日期，来自集合的数学导入namedtuple导入json DB_out_host=‘172.16.170.81’DB_OUT_PORT=3308 DB_OUT_USER=‘zhutar’DB_OUT_PWD=‘zhutao 321’DB_out_name=‘WPI_DEMO’Engine=Create_Engine(‘MySQL pymysql：//%(user)s：%(pwd)s@%(host)s：%(port)s/%(dbname)s?charset=utf8‘%{’user‘：DB_OUT_USER，‘pwd’：DB_out_PWD，‘host’：DB_OUT_HOST，‘dbName’：DB_OUT_NAME，‘port’：DB_OUT_PORT}，编码=‘utf-8’)conn=工程连接()df_main=pd.read_sql_table(‘t_SUMERY_INDEX’，Conn)",
    "df_theme": "df_theme",
    "np.random.uniform(38.27, 63.15, size=36)": "制服(38.27，63.15，尺寸=36)",
    "hot = np.random.uniform(38.27, 63.15, size=36)": "均匀(38.27，63.15，大小=36)",
    "pandas.date_range(": "日期范围(",
    "pd.date_range(start='1/1/2018', end='1/08/2018')": "pd.date_range(Start=‘1/1/2018’，end=‘1/08/2018’)",
    "pd.date_range(start='2018-04-24', end='2018-04-27', periods=3)": "日期范围(Start=‘2018-04-24’，End=‘2018-04-27’，周期=3)",
    "2015-11-01": "2015-11-01",
    "freq='M'": "freq='M'",
    "periods": "n. 周期;时期( period的名词复数 );（一段）时间;学时;句号;",
    " freq='MS'": "freq='MS'",
    "opposition": "n. （强烈的）反对，敌对;（政府的）反对党，在野党;对抗，抵抗;相对，对照;\nadj. 反对的，对抗的;",
    "'hot',": "'hot',",
    "suppot": "其实是支持",
    "time": "n. 时间;时刻;时代;次;\nvt. 为…安排时间;测定…的时间;调准（机械的）速度;拨准（钟、表）的快慢;\nvi. 合拍;和谐;打拍子;\nadj. 定时的;定期的;[美国英语]分期（付款）的;\nint. [体育]时间到，（一场或一局等的）比赛时限，暂停;",
    "new": "adj. 新的，崭新的;新鲜的，新到的;现代的;初次（听到）的;\nadv. 新近，最近;",
    "dates.apply(lambda x: x.strftime('%Y-%m-%d'))": "日期.应用(lambda x：x.strftime(‘%Y-%m-%d’)",
    "new['time']": "new['time']",
    ")": ")",
    "new['id']": "new['id']",
    "hot": "adj. 热的;辣的;激动的;热门的;\nvt. 使兴奋，使激动;\nadv. 热的;紧迫的;\nvi. 变热;",
    "\t2016-06-01": "2016-06-01",
    "import re\nre.search('\\d\\d\\d\\d-06-\\d\\d','\t2016-06-01')": "重新输入re.search(‘\\d\\d-\\d’，‘2016-06-01’)",
    "re.search('\\d\\d\\d\\d-06-\\d\\d','\t2016-06-01')": "再搜索(‘\\d\\d-06-\\d’，‘2016-06-01’)",
    "{4}": "{4}",
    "re.search('\\d{4}-06-\\d{2}','\t2016-06-01')": "再搜索(‘\\d{4}-06-\\d{2}’，‘2016-06-01’)",
    "np.random.uniform(5, 10)": "制服(5，10)",
    "'sort=True": "排序=真",
    "sort=True'": "排序=真",
    "'sort=True'": "“排序=真”",
    "data_out.to_sql('t_associate_index', con=engine, index=False, if_exists='append')": "data_out.to_sql(‘t_Associate_index’，con=Engine，index=false，if_ist=‘append’)",
    "replace": "vt. 替换;代替;把…放回原位;（用…）替换;",
    "out": "adv. 出局;在外，在外部;完全，彻底;出版;\nprep. （表示来源）从;（从…里）出来;（表示不在原状态）脱离;离去;\nvt. 使熄灭;揭露;驱逐;\nadj. 外面的;出局的;下台的;外围的;\nn. 不流行;出局;",
    "support": "vt. 支持;帮助;支撑;维持;\nn. 支撑;支持者;[数学]支集;支撑物;",
    "new['time'] = new['time'].apply(lambda x: x.strftime('%Y-%m-%d'))": "[‘time’]=新[‘time’].应用(lambda x：x.strftime(‘%Y-%m-%d’)",
    "new['topic'] = '教育'": "New ['topic'] =' education'",
    "new = new.reset_index()": "New=new.重置_index()",
    "new = new.rename({'index':'id'},axis=1)": "New=new.rename({‘index’：‘id’}，Axis=1)",
    "new['id']= new['id']+70": "新[‘id’]=新[‘id’]70",
    "import re\ndef f(x):\n    if re.search('\\d{4}-06-\\d{2}',x['time']):\n        x['hot'] = x['hot']+np.random.uniform(5, 10)\n    return x\nnew = new.apply(f,axis=1)": "Importredeff(x):ifre.search(\"d'4}-06-d#2})\",x[\"time\"]:x[“热”\"]=x[\"热\"]+NP.Random.Uniform(5,10)returnxnew=new.apply(f,axis=1)",
    "out = pd.concat([df_theme, new],sort=True)": "out=pd.conat([df_main，New]，Sort=True)",
    "time = pd.date_range(start='2015-11-01', end='2018-10-01', freq='MS')": "Time=pd.date_range(Start=‘2015-11-01’，end=‘2018-10-01’，freq=‘ms’)",
    "engine": "n. 发动机，引擎;工具;火车头;机车;\nvt. 给…安装发动机;",
    "uniform": "n. 制服;军服;通讯中用以代表字母 u 的词;\nadj. （形状，性质等）一样的;规格一致的;始终如一的;\nv. 使规格一律;使均一;使穿制服;",
    "np.random": "np.随机",
    "np.round(hot, 2)": "np.圆(热，2)",
    "np.round(d, 2)": "np.轮(d，2)",
    "    d = np.round(d, 2)": "D=np.轮(d，2)",
    "lst": "abbr. list 清单;liquid-oxygen start tank 液氧起动箱;living structures tank 活动结构箱;large space telescope 大型空间望远镜;",
    "\nfor col in ['']": "用于[‘]",
    ".commit()": ".犯罪，做错事()",
    "estige": "estige",
    "To embed the networks by preserving both the first-order\nand second-order proximity, a simple and effective way we\nfind in practice is to train the LINE model which preserves\nthe first-order proximity and second-order proximity sepa-\nrately and then concatenate the embeddings trained by the\ntwo methods for each vertex. A more principled way to\ncombine the two proximity is to jointly train the objective\nfunction (3) and (6), which we leave as future work.": "为了通过保留一阶和二阶邻近来嵌入网络，我们在实践中发现的一种简单而有效的方法是训练分别保持一阶接近和二阶接近的线模型，然后将这两种方法训练的嵌入连接到每个顶点。将两者结合在一起的一个更有原则的方法是联合训练目标函数(3)和(6)，这是我们在今后的工作中留下的。",
    "Optimizing objective (6) is computationally expensive,\nwhich requires the summation over the entire set of ver-\ntices when calculating the conditional probability p 2 (·|v i ).\nTo address this problem, we adopt the approach of negative\nsampling proposed in [13], which samples multiple negative\nedges according to some noisy distribution for each edge\n(i, j). More specifically, it specifies the following objective\nfunction for each edge (i, j):": "优化目标(6)计算代价较高，在计算条件概率p2(·vi)时，需要对整个顶点集进行求和。为了解决这一问题，我们采用了文献[13]中提出的负采样方法，根据每条边(i，j)的某些噪声分布对多个负边进行采样。更具体地说，它为每一条边(i，j)指定了以下目标函数：",
    "proposed": "adj. 被提议的，所推荐的;\nv. 提议，建议( propose的过去式和过去分词 );打算;提供（解释）;提出（行动，计划或供表决的方案等）;",
    "contrastive": "adj. 对比的;",
    "import os\nimport sys\nfrom dateutil.parser import parse\nimport pandas as pd\nimport numpy as np\nimport datetime as dt": "从dateutil.解析器导入os导入系统解析导入熊猫为PD导入numpy作为NP导入日期时间为dt",
    "import numpy as np \nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nplt.style.use('fivethirtyeight')\nimport warnings\nwarnings.filterwarnings('ignore')\n%matplotlib inline": "输入numpy作为NP导入熊猫作为pd导入matplotlib.pyart作为PLT导入海运作为SNS plt.style.use(‘Five3tyEight’)导入警告。过滤警告(‘忽略’)%matplotlib内联",
    "\nimport numpy as np \nimport pandas as pd": "进口Numpy作为NP进口熊猫作为PD",
    " as dt": "DT",
    "consum = pd.read_csv('consumption.txt')": "consum=pd.read_csv(‘sumption.txt’)",
    "consum.drop(['id'], axis=1,inplace=True)\nconsum.fillna('',inplace=True)": "([‘id’]，轴=1，Inplace=True)",
    "634d0b9560048f6ea50df1d5fb854401f2e51e0a839eefd7": "634d0b9560048f6ea50df1d5fb854401f2e51e0a839eefd7",
    "consum.fillna('',inplace=True)\n": "填写(‘，inplace=True)",
    "consum": "consumption消费",
    "3803060": "三百八十零万三千零六十",
    "Jasmine Flower": " 茉莉花;",
    "Literal": "adj. 照字面的;原义的;逐字的;平实的，避免夸张;\nn. [印]错排，文字上的错误;",
    "def input_fn(data_file, num_epochs, shuffle, batch_size, is_classification=False):\n  \"\"\"Generate an input function for the Estimator.\"\"\"\n  assert tf.gfile.Exists(data_file), (\n      '%s not found. Please make sure you have run census_dataset.py and '\n      'set the --data_dir argument to the correct path.' % data_file)\n\n  def parse_csv(value):\n    tf.logging.info('Parsing {}'.format(data_file))\n    record_defaults = [[0.0] if c in continus_features+['totals.transactionRevenue'] else [''] for c in _CSV_COLUMNS]\n    columns = tf.decode_csv(value, record_defaults)\n    features = dict(zip(_CSV_COLUMNS, columns))\n    labels = features.pop('totals.transactionRevenue')\n    if is_classification:\n        classes = tf.math.logical_not(tf.equal(labels, 0))  # binary classification\n        return features, classes\n    else:\n        return features, labels\n\n  # Extract lines from input files using the Dataset API.\n  dataset = tf.data.TextLineDataset(data_file)\n\n  if shuffle:\n    dataset = dataset.shuffle(buffer_size=_NUM_EXAMPLES['train'])\n\n  dataset = dataset.map(parse_csv, num_parallel_calls=5)\n\n  # We call repeat after shuffling, rather than before, to prevent separate\n  # epochs from blending together.\n  dataset = dataset.repeat(num_epochs)\n  dataset = dataset.batch(batch_size)\n  return dataset": "def input_fn(data_file，num_EIRCHS，霉变，批大小，is_class=false)：“为估计器生成一个输入函数”。断言tf.gfile.Exist(Data_File)，(‘%s未找到)。请确保您已经运行了普查_data et.py和‘将-data_dir参数设置为正确的路径。’%data_file)def解析_csv(值)：tf.logging.info(“解析{}”格式(Data_File)记录_DEFAULS=[0.0]如果c在CONLAUS_FACTS[‘AGEL.TRANSTUCTS’]Other[‘]for c in_CSV_Columns]列=tf.decode_CSV(值，函数=dict(zip(_csv_Columns，列)标签=filures.op(‘Total s.transaction年收入’)，如果is_class：Class=tf.math.logicalno(tf.equent(标签，0)#二进制分类返回特性，其他类：使用DataSet API返回特性、标签#从输入文件中提取行。DataSet=tf.data.TextLineDataset(Data_File)如果洗牌：DataSet=dataset.shuffle(buffer_size=_NUM_EXAMPLES[‘train’])DataSet=data et.map(解析_csv，num_paring_calls=5)#我们在洗牌后调用重复，而不是以前，以防止单独的#时代混合在一起。数据集=数据集.重复数据集=数据集.批处理(批处理_大小)返回数据集",
    "#!/usr/bin/env python\n# -*- coding: utf-8 -*-\n\n__author__ = 'liwenjie'\n\nimport tensorflow as tf\nimport pandas as pd\nimport numpy as np\nimport numbers\n\nimport sys\nsys.path.append('/home/liwenjie/liwenjie/projects/kagglegacrp')\nfrom tfprocess.wddataset import build_model_columns\nfrom tfprocess.wddataset import input_fn\nfrom tfprocess.logger import BaseBenchmarkLogger\nfrom tfprocess.wdnet import past_stop_threshold\ntf.logging.set_verbosity(tf.logging.INFO)\n\n\ntest_file = '/home/liwenjie/liwenjie/projects/kagglegacrp/input/tf_test.csv'\neva_file = '/home/liwenjie/liwenjie/projects/kagglegacrp/input/tf_val.csv'\ntrain_file = '/home/liwenjie/liwenjie/projects/kagglegacrp/input/tf_train.csv'\ntrain = pd.read_csv('/home/liwenjie/liwenjie/projects/kagglegacrp/input/tf_train_all.csv')\ntrain_features = [_f for _f in train.columns if _f is not 'totals.transactionRevenue']\n\nbatch_size = 128\ntrain_steps = 2 * 722546\nsteps_between_evals = 5000\nwide_columns, deep_columns = build_model_columns()\nhidden_units = [1500, 500, 150, 50]\nrun_config = tf.estimator.RunConfig().replace(session_config=tf.ConfigProto(device_count={\"CPU\":4, 'GPU': 0},\n                                                                            inter_op_parallelism_threads=0,\n                                                                            intra_op_parallelism_threads=0))\nestimator = tf.estimator.DNNLinearCombinedRegressor(model_dir='/tmp/gacrp_model',\n                                                    linear_feature_columns=wide_columns,\n                                                    linear_optimizer='Ftrl',\n                                                    dnn_feature_columns=deep_columns,\n                                                    dnn_optimizer='Adagrad',\n                                                    dnn_hidden_units=hidden_units,\n                                                    dnn_activation_fn=tf.nn.relu,\n                                                    dnn_dropout=None,\n                                                    label_dimension=1,\n                                                    weight_column=None,\n                                                    input_layer_partitioner=None,\n                                                    config=run_config,\n                                                    warm_start_from=None,\n                                                    loss_reduction=tf.losses.Reduction.SUM,\n                                                    batch_norm=False,\n                                                    linear_sparse_combiner='sum')\nestimator_c = tf.estimator.DNNLinearCombinedClassifier(model_dir='/tmp/gacrp_model_c',\n                                                       linear_feature_columns=wide_columns,\n                                                       linear_optimizer='Ftrl',\n                                                       dnn_feature_columns=deep_columns,\n                                                       dnn_optimizer='Adagrad',\n                                                       dnn_hidden_units=hidden_units,\n                                                       dnn_activation_fn=tf.nn.relu,\n                                                       dnn_dropout=None,\n                                                       n_classes=2,\n                                                       weight_column=None,\n                                                       label_vocabulary=None,\n                                                       input_layer_partitioner=None,\n                                                       config=None,\n                                                       warm_start_from=None,\n                                                       loss_reduction=tf.losses.Reduction.SUM,\n                                                       batch_norm=False,\n                                                       linear_sparse_combiner='sum')\n\neva_input_fn = lambda: input_fn(eva_file, 1, False, batch_size)\ntrain_input_fn = lambda: input_fn(train_file, 50, True, batch_size)\ntest_input_fn = lambda: input_fn(test_file, 1, False, batch_size)\neva_input_fn_c = lambda: input_fn(eva_file, 1, False, batch_size, is_classification=True)\ntrain_input_fn_c = lambda: input_fn(train_file, 50, True, batch_size, is_classification=True)\ntest_input_fn_c = lambda: input_fn(test_file, 1, False, batch_size, is_classification=True)\n\ntensors_to_log = {\n    'average_loss': 'head/truediv',\n    'loss': 'head/weighted_loss/Sum'\n}\ntrain_hooks = [tf.train.LoggingTensorHook(tensors=tensors_to_log, every_n_iter=100)]\n# train_hooks = train_hooks + [tf_debug.LocalCLIDebugHook()]\n# train_hooks = train_hooks + [tf_debug.TensorBoardDebugHook(\"localhost:6064\")]\nearly_stop = False\nrun_params = {\n    'batch_size': batch_size,\n    'train_steps': train_steps,\n    'model_type': 'wide_deep',\n}\nbenchmark_logger = BaseBenchmarkLogger()\nbenchmark_logger.log_run_info('wide_deep', \"Census Income\", run_params, test_id=None)\nfor n in range(train_steps // steps_between_evals):\n    estimator_c.train(input_fn=train_input_fn_c, hooks=train_hooks, steps=500)\n    results = estimator_c.evaluate(input_fn=eva_input_fn_c)\n    # Display evaluation metrics\n    tf.logging.info('Results at step %d / %d', (n + 1) * steps_between_evals, train_steps)\n    tf.logging.info('-' * 60)\n    for key in sorted(results):\n        tf.logging.info('%s: %s' % (key, results[key]))\n    benchmark_logger.log_evaluation_result(results)\n    if early_stop and past_stop_threshold(None, results['loss']):\n        break\nres = estimator_c.predict(test_input_fn)\nlres = [p['predictions'] for p in res]\nnres = np.array(lres)\nnp.save('nnres_c.npy', nres)\n": "",
    "from tfprocess.wddataset import build_model_columns\nfrom tfprocess.wddataset import input_fn\nfrom tfprocess.logger import BaseBenchmarkLogger\nfrom tfprocess.wdnet import past_stop_threshold\ntf.logging.set_verbosity(tf.logging.INFO)\n\n\ntest_file = '/home/liwenjie/liwenjie/projects/kagglegacrp/input/tf_test.csv'\neva_file = '/home/liwenjie/liwenjie/projects/kagglegacrp/input/tf_val.csv'\ntrain_file = '/home/liwenjie/liwenjie/projects/kagglegacrp/input/tf_train.csv'\ntrain = pd.read_csv('/home/liwenjie/liwenjie/projects/kagglegacrp/input/tf_train_all.csv')\ntrain_features = [_f for _f in train.columns if _f is not 'totals.transactionRevenue']\n\nbatch_size = 128\ntrain_steps = 2 * 722546\nsteps_between_evals = 5000\nwide_columns, deep_columns = build_model_columns()\nhidden_units = [1500, 500, 150, 50]\nrun_config = tf.estimator.RunConfig().replace(session_config=tf.ConfigProto(device_count={\"CPU\":4, 'GPU': 0},\n                                                                            inter_op_parallelism_threads=0,\n                                                                            intra_op_parallelism_threads=0))\nestimator = tf.estimator.DNNLinearCombinedRegressor(model_dir='/tmp/gacrp_model',\n                                                    linear_feature_columns=wide_columns,\n                                                    linear_optimizer='Ftrl',\n                                                    dnn_feature_columns=deep_columns,\n                                                    dnn_optimizer='Adagrad',\n                                                    dnn_hidden_units=hidden_units,\n                                                    dnn_activation_fn=tf.nn.relu,\n                                                    dnn_dropout=None,\n                                                    label_dimension=1,\n                                                    weight_column=None,\n                                                    input_layer_partitioner=None,\n                                                    config=run_config,\n                                                    warm_start_from=None,\n                                                    loss_reduction=tf.losses.Reduction.SUM,\n                                                    batch_norm=False,\n                                                    linear_sparse_combiner='sum')\nestimator_c = tf.estimator.DNNLinearCombinedClassifier(model_dir='/tmp/gacrp_model_c',\n                                                       linear_feature_columns=wide_columns,\n                                                       linear_optimizer='Ftrl',\n                                                       dnn_feature_columns=deep_columns,\n                                                       dnn_optimizer='Adagrad',\n                                                       dnn_hidden_units=hidden_units,\n                                                       dnn_activation_fn=tf.nn.relu,\n                                                       dnn_dropout=None,\n                                                       n_classes=2,\n                                                       weight_column=None,\n                                                       label_vocabulary=None,\n                                                       input_layer_partitioner=None,\n                                                       config=None,\n                                                       warm_start_from=None,\n                                                       loss_reduction=tf.losses.Reduction.SUM,\n                                                       batch_norm=False,\n                                                       linear_sparse_combiner='sum')\n\neva_input_fn = lambda: input_fn(eva_file, 1, False, batch_size)\ntrain_input_fn = lambda: input_fn(train_file, 50, True, batch_size)\ntest_input_fn = lambda: input_fn(test_file, 1, False, batch_size)\neva_input_fn_c = lambda: input_fn(eva_file, 1, False, batch_size, is_classification=True)\ntrain_input_fn_c = lambda: input_fn(train_file, 50, True, batch_size, is_classification=True)\ntest_input_fn_c = lambda: input_fn(test_file, 1, False, batch_size, is_classification=True)\n\ntensors_to_log = {\n    'average_loss': 'head/truediv',\n    'loss': 'head/weighted_loss/Sum'\n}\ntrain_hooks = [tf.train.LoggingTensorHook(tensors=tensors_to_log, every_n_iter=100)]\n# train_hooks = train_hooks + [tf_debug.LocalCLIDebugHook()]\n# train_hooks = train_hooks + [tf_debug.TensorBoardDebugHook(\"localhost:6064\")]\nearly_stop = False\nrun_params = {\n    'batch_size': batch_size,\n    'train_steps': train_steps,\n    'model_type': 'wide_deep',\n}\nbenchmark_logger = BaseBenchmarkLogger()\nbenchmark_logger.log_run_info('wide_deep', \"Census Income\", run_params, test_id=None)\nfor n in range(train_steps // steps_between_evals):\n    estimator_c.train(input_fn=train_input_fn_c, hooks=train_hooks, steps=500)\n    results = estimator_c.evaluate(input_fn=eva_input_fn_c)\n    # Display evaluation metrics\n    tf.logging.info('Results at step %d / %d', (n + 1) * steps_between_evals, train_steps)\n    tf.logging.info('-' * 60)\n    for key in sorted(results):\n        tf.logging.info('%s: %s' % (key, results[key]))\n    benchmark_logger.log_evaluation_result(results)\n    if early_stop and past_stop_threshold(None, results['loss']):\n        break\nres = estimator_c.predict(test_input_fn)\nlres = [p['predictions'] for p in res]\nnres = np.array(lres)\nnp.save('nnres_c.npy', nres)\n": "",
    "from tfprocess.wddataset import build_model_columns\nfrom tfprocess.wddataset import input_fn\nfrom tfprocess.logger import BaseBenchmarkLogger\nfrom tfprocess.wdnet import past_stop_threshold": "从tfprocess.wdDataSet导入Build_Model_Columns，从tfprocess.wdDataSet导入输入_fn，从tfprocess.logger导入BaseBenchmarkLogger，从tfprocess.wdnet导入",
    "\ntf.logging.set_verbosity(tf.logging.INFO)\n": "tf.logging.set_verbosity(tf.logging.INFO)",
    "class BaseBenchmarkLogger(object):\n    \"\"\"Class to log the benchmark information to STDOUT.\"\"\"\n\n    def log_evaluation_result(self, eval_results):\n        \"\"\"Log the evaluation result.\n    \n        The evaluate result is a dictionary that contains metrics defined in\n        model_fn. It also contains a entry for global_step which contains the value\n        of the global step when evaluation was performed.\n    \n        Args:\n          eval_results: dict, the result of evaluate.\n        \"\"\"\n        if not isinstance(eval_results, dict):\n            tf.logging.warning(\"eval_results should be dictionary for logging. \"\n                               \"Got %s\", type(eval_results))\n            return\n        global_step = eval_results[tf.GraphKeys.GLOBAL_STEP]\n        for key in sorted(eval_results):\n            if key != tf.GraphKeys.GLOBAL_STEP:\n                self.log_metric(key, eval_results[key], global_step=global_step)\n\n    def log_metric(self, name, value, unit=None, global_step=None, extras=None):\n        \"\"\"Log the benchmark metric information to local file.\n    \n        Currently the logging is done in a synchronized way. This should be updated\n        to log asynchronously.\n    \n        Args:\n          name: string, the name of the metric to log.\n          value: number, the value of the metric. The value will not be logged if it\n            is not a number type.\n          unit: string, the unit of the metric, E.g \"image per second\".\n          global_step: int, the global_step when the metric is logged.\n          extras: map of string:string, the extra information about the metric.\n        \"\"\"\n        metric = _process_metric_to_json(name, value, unit, global_step, extras)\n        if metric:\n            tf.logging.info(\"Benchmark metric: %s\", metric)\n\n    def log_run_info(self, model_name, dataset_name, run_params, test_id=None):\n        tf.logging.info(\"Benchmark run: %s\",\n                        _gather_run_info(model_name, dataset_name, run_params,\n                                         test_id))\n\n    def on_finish(self, status):\n        pass": "类BaseBenchmarkLogger(Object)：“类将基准信息记录到STDOUT。DEF LOG_COOLATION_COMPUES(Self，val_Results)：“记录评估结果。评估结果是一个字典，其中包含模型_fn中定义的度量。它还包含全局_STEP的一个条目，其中包含执行评估时全局步骤的值。ARGS：评价结果：DECT，评价结果。“”如果不是ISINENT，则为ISINEINE(val_Results，dict)：tf.logging.Warning(“val_Results应该是用于日志记录的字典”)。“有%s”，为排序中的键返回全局_STEP=val_STEP[tf.GraphKeys.GLOBAL_STEP]：if key！=tf.GraphKeys.GLOBAL_STEP：自.log度量(密钥，Eval_REAS[key]，Global_STEP=Global_STEP)def LOG_米制(Self，Name，Value，Unit=NONE，Global_STEP=NONE，EXPAS=NONE)：“将基准度量信息记录到本地文件中。目前，日志记录是以同步方式完成的。应该将其更新为异步日志。args：name：string，要记录的度量的名称。数值：数字，度量的值。如果该值不是数字类型，则不会记录该值。单位：字符串，度量的单位，例如“图像每秒”。全局_STEP：int，在记录度量时的全局_STEP。附加：字符串映射：String，关于度量的额外信息。“”如果度量：tf.logging.info(“基准度量：%s”，度量)def log_run_info(Self，model_name，DataSet_name，Run_params，test_id=None)：tf.logging.info(“Benchmark Run：%s”，_Collection_run_info(Model_name，DataSet_name，run_params)，)完成(自我，状态)：通过",
    "def _process_metric_to_json(\n        name, value, unit=None, global_step=None, extras=None):\n    \"\"\"Validate the metric data and generate JSON for insert.\"\"\"\n    if not isinstance(value, numbers.Number):\n        tf.logging.warning(\n            \"Metric value to log should be a number. Got %s\", type(value))\n        return None\n\n    extras = _convert_to_json_dict(extras)\n    return {\n        \"name\": name,\n        \"value\": float(value),\n        \"unit\": unit,\n        \"global_step\": global_step,\n        \"timestamp\": datetime.datetime.utcnow().strftime(\n            _DATE_TIME_FORMAT_PATTERN),\n        \"extras\": extras}": "def_process_mi_to_json(名称、值、单位=无、全局_步骤=无、附加值=无)：“验证度量数据并为插入生成JSON”。如果不是isinstation(value，number s.Number)：tf.logging.Warning(“日志的度量值应该是一个数字”)。获取%s“，类型(值)不返回任何附加项=_转换_json_dict(附加)返回{”名称“：名称，”值“：浮动(值)，”单位“：单位，“全局_STEP”：全局_STEP，“时间戳”：datetime.utcNow().strftime(_Date_Time_Format_Pattern)，“EXPAS”：EXPAS}",
    "_gather_run_info": "_",
    "logger.py": "logger.py",
    "#!/usr/bin/env python\n# -*- coding: utf-8 -*-\n\n__author__ = 'liwenjie'\n\nimport os\nimport sys\nfrom dateutil.parser import parse\nimport pandas as pd\nimport numpy as np\nimport datetime as dt\nimport tensorflow as tf\n\ntf.logging.set_verbosity(tf.logging.INFO)\n\n\ndef input_fn(data_file, num_epochs, shuffle, batch_size, is_classification=False):\n    \"\"\"Generate an input function for the Estimator.\"\"\"\n    assert tf.gfile.Exists(data_file), (\n        '%s not found. Please make sure you have run census_dataset.py and '\n        'set the --data_dir argument to the correct path.' % data_file)\n\n    def parse_csv(value):\n        tf.logging.info('Parsing {}'.format(data_file))\n        record_defaults = [[0.0] if c in continus_features + ['totals.transactionRevenue'] else [''] for c in\n                           _CSV_COLUMNS]\n        columns = tf.decode_csv(value, record_defaults)\n        features = dict(zip(_CSV_COLUMNS, columns))\n        labels = features.pop('totals.transactionRevenue')\n        if is_classification:\n            classes = tf.math.logical_not(tf.equal(labels, 0))  # binary classification\n            return features, classes\n        else:\n            return features, labels\n\n    # Extract lines from input files using the Dataset API.\n    dataset = tf.data.TextLineDataset(data_file)\n\n    if shuffle:\n        dataset = dataset.shuffle(buffer_size=_NUM_EXAMPLES['train'])\n\n    dataset = dataset.map(parse_csv, num_parallel_calls=5)\n\n    # We call repeat after shuffling, rather than before, to prevent separate\n    # epochs from blending together.\n    dataset = dataset.repeat(num_epochs)\n    dataset = dataset.batch(batch_size)\n    return dataset\n\n\ntest_file = '/home/liwenjie/liwenjie/projects/kagglegacrp/input/tf_test.csv'\neva_file = '/home/liwenjie/liwenjie/projects/kagglegacrp/input/tf_val.csv'\ntrain_file = '/home/liwenjie/liwenjie/projects/kagglegacrp/input/tf_train.csv'\ntrain = pd.read_csv('/home/liwenjie/liwenjie/projects/kagglegacrp/input/tf_train_all.csv')\ntrain_features = [_f for _f in train.columns if _f is not 'totals.transactionRevenue']\n\nbatch_size = 128\ntrain_steps = 2 * 722546\nsteps_between_evals = 5000\nwide_columns, deep_columns = build_model_columns()\nhidden_units = [1500, 500, 150, 50]\nrun_config = tf.estimator.RunConfig().replace(session_config=tf.ConfigProto(device_count={\"CPU\": 4, 'GPU': 0},\n                                                                            inter_op_parallelism_threads=0,\n                                                                            intra_op_parallelism_threads=0))\nestimator = tf.estimator.DNNLinearCombinedRegressor(model_dir='/tmp/gacrp_model',\n                                                    linear_feature_columns=wide_columns,\n                                                    linear_optimizer='Ftrl',\n                                                    dnn_feature_columns=deep_columns,\n                                                    dnn_optimizer='Adagrad',\n                                                    dnn_hidden_units=hidden_units,\n                                                    dnn_activation_fn=tf.nn.relu,\n                                                    dnn_dropout=None,\n                                                    label_dimension=1,\n                                                    weight_column=None,\n                                                    input_layer_partitioner=None,\n                                                    config=run_config,\n                                                    warm_start_from=None,\n                                                    loss_reduction=tf.losses.Reduction.SUM,\n                                                    batch_norm=False,\n                                                    linear_sparse_combiner='sum')\nestimator_c = tf.estimator.DNNLinearCombinedClassifier(model_dir='/tmp/gacrp_model_c',\n                                                       linear_feature_columns=wide_columns,\n                                                       linear_optimizer='Ftrl',\n                                                       dnn_feature_columns=deep_columns,\n                                                       dnn_optimizer='Adagrad',\n                                                       dnn_hidden_units=hidden_units,\n                                                       dnn_activation_fn=tf.nn.relu,\n                                                       dnn_dropout=None,\n                                                       n_classes=2,\n                                                       weight_column=None,\n                                                       label_vocabulary=None,\n                                                       input_layer_partitioner=None,\n                                                       config=None,\n                                                       warm_start_from=None,\n                                                       loss_reduction=tf.losses.Reduction.SUM,\n                                                       batch_norm=False,\n                                                       linear_sparse_combiner='sum')\n\neva_input_fn = lambda: input_fn(eva_file, 1, False, batch_size)\ntrain_input_fn = lambda: input_fn(train_file, 50, True, batch_size)\ntest_input_fn = lambda: input_fn(test_file, 1, False, batch_size)\neva_input_fn_c = lambda: input_fn(eva_file, 1, False, batch_size, is_classification=True)\ntrain_input_fn_c = lambda: input_fn(train_file, 50, True, batch_size, is_classification=True)\ntest_input_fn_c = lambda: input_fn(test_file, 1, False, batch_size, is_classification=True)\n\ntensors_to_log = {\n    'average_loss': 'head/truediv',\n    'loss': 'head/weighted_loss/Sum'\n}\ntrain_hooks = [tf.train.LoggingTensorHook(tensors=tensors_to_log, every_n_iter=100)]\n# train_hooks = train_hooks + [tf_debug.LocalCLIDebugHook()]\n# train_hooks = train_hooks + [tf_debug.TensorBoardDebugHook(\"localhost:6064\")]\nearly_stop = False\nrun_params = {\n    'batch_size': batch_size,\n    'train_steps': train_steps,\n    'model_type': 'wide_deep',\n}\nbenchmark_logger = BaseBenchmarkLogger()\nbenchmark_logger.log_run_info('wide_deep', \"Census Income\", run_params, test_id=None)\nfor n in range(train_steps // steps_between_evals):\n    estimator_c.train(input_fn=train_input_fn_c, hooks=train_hooks, steps=500)\n    results = estimator_c.evaluate(input_fn=eva_input_fn_c)\n    # Display evaluation metrics\n    tf.logging.info('Results at step %d / %d', (n + 1) * steps_between_evals, train_steps)\n    tf.logging.info('-' * 60)\n    for key in sorted(results):\n        tf.logging.info('%s: %s' % (key, results[key]))\n    benchmark_logger.log_evaluation_result(results)\n    if early_stop and past_stop_threshold(None, results['loss']):\n        break\nres = estimator_c.predict(test_input_fn)\nlres = [p['predictions'] for p in res]\nnres = np.array(lres)\nnp.save('nnres_c.npy', nres)\n": "",
    "  return dataset": "返回数据集",
    "wddataset.py": "wddata et.py",
    "input_fn": "输入FN",
    "dataset": " [电] 资料组;",
    "dataset.": "[电] 资料组.",
    "logger": "n. <美>樵夫;伐木工;记录器;",
    "logger.": "logger.",
    "past_stop_threshold": "过停阈值",
    "def past_stop_threshold(stop_threshold, eval_metric):\n  \"\"\"Return a boolean representing whether a model should be stopped.\n\n  Args:\n    stop_threshold: float, the threshold above which a model should stop\n      training.\n    eval_metric: float, the current value of the relevant metric to check.\n\n  Returns:\n    True if training should stop, False otherwise.\n\n  Raises:\n    ValueError: if either stop_threshold or eval_metric is not a number\n  \"\"\"\n  if stop_threshold is None:\n    return False\n\n  if not isinstance(stop_threshold, numbers.Number):\n    raise ValueError(\"Threshold for checking stop conditions must be a number.\")\n  if not isinstance(eval_metric, numbers.Number):\n    raise ValueError(\"Eval metric being checked against stop conditions \"\n                     \"must be a number.\")\n\n  if eval_metric >= stop_threshold:\n    tf.logging.info(\n        \"Stop threshold of {} was passed with metric value {}.\".format(\n            stop_threshold, eval_metric))\n    return True\n\n  return False": "DEFPASS_STEST_RENTOR(STOP_ALTERNAT，val_度量)：“返回一个布尔值，表示模型是否应该停止。ARGS：停止阈值：浮动，模型应该停止训练的阈值。Eval_米制：浮动，要检查的相关度量的当前值。返回：如果训练停止，则为true，否则为false。引发：ValueError：如果Stop_阈值或val_米制不是数字“，如果Stop_阈值为None：返回false如果不是isinstation(STOP_阈值，编号.Number)：RANValueError(”检查停止条件的阈值必须是一个数字“)。如果不是ISINEINECT，则为ISINEINE(val_MARECTION，Number)：引发ValueError(“根据停止条件检查Eval度量”)“必须是一个数字”。如果EVA_METRIAL>=STOP_RANERATE：tf.logging.info(“{}的停止阈值用米标值{}传递”.格式(STOP_阈值，val_METRIAL)返回True False",
    "威妥玛": "威妥玛拼音（英文：Wade-Giles system），习惯称作威妥玛拼法或威玛式拼音、韦氏拼音、威翟式拼音，是一套用于拼写中文普通话的罗马拼音系统。19世纪中叶由英国人威妥玛(Thomas Francis Wade)发明，后由翟理斯(Herbert Allen Giles)完成修订，并编入其所撰写的汉英字典。 ",
    "lǐ]": "lǐ]",
    "wén": "Wn",
    "jié": "jime",
    "li3wen2chieh2": "li3wen2chieh2",
    "yüan k`an": "YourAK\"",
    "liwenchieh": "liwenchieh",
    "li": "abbr. Liechtenstein 列支敦士登;",
    "威妥玛拼音": "Wade-Giles romanization ",
    "data/": "数据",
    "__file__": "文件",
    "os.path.realpath(__file__)": "os.path.realpath(_File_)",
    "\nos.path.": "通径。",
    "os.path.dirname(os.path.realpath(__file__))": "domname(os.path.realpath(_File_)",
    "SKCompat": "compat兼容性",
    "lstm_model": "LSTM模型",
    "from tensorflow.contrib.learn.python.learn.estimators.estimator import SKCompat": "从tensorflow.contrib.learn.python.learn.estimators.estimator导入SKCompat",
    "https://www.tensorflow.org/code/tensorflow/contrib/learn/README.md": "https：/www.tensorflow.org/code/TensorFlow/cont肋骨/Learning/README.md",
    "tf.estimator": "tf.估计量",
    "tensorflow": "tensorflow",
    ".Estimator": ".评价者，评估特有者",
    "tf.estimator.Estimator": "估计值",
    "tf": "abbr. French Southern Territories 法国南部;",
    "Estimator": "n. 评价者，评估特有者;估测者;",
    "regressor": "n. 回归量;",
    "'''定义LSTM模型'''\ndef lstm_model(X, y):\n    '''以前面定义的LSTM cell为基础定义多层堆叠的LSTM，我们这里只有1层'''\n    cell = rnn.MultiRNNCell([LstmCell() for _ in range(NUM_LAYERS)])\n\n    '''将已经堆叠起的LSTM单元转化成动态的可在训练过程中更新的LSTM单元'''\n    output, _ = tf.nn.dynamic_rnn(cell, X, dtype=tf.float32)\n\n    '''根据预定义的每层神经元个数来生成隐层每个单元'''\n    output = tf.reshape(output, [-1, HIDDEN_SIZE])\n\n    '''通过无激活函数的全连接层计算线性回归，并将数据压缩成一维数组结构'''\n    predictions = tf.contrib.layers.fully_connected(output, 1, None)\n\n    '''统一预测值与真实值的形状'''\n    labels = tf.reshape(y, [-1])\n    predictions = tf.reshape(predictions, [-1])\n\n    '''定义损失函数，这里为正常的均方误差'''\n    loss = tf.losses.mean_squared_error(predictions, labels)\n\n    '''定义优化器各参数'''\n    train_op = tf.contrib.layers.optimize_loss(loss,\n                                               tf.contrib.framework.get_global_step(),\n                                               optimizer='Adagrad',\n                                               learning_rate=0.6)\n    '''返回预测值、损失函数及优化器'''\n    return predictions, loss, train_op": "'define the LSTM model' def lstm_model (X, y): 'define a multilayered LSTM, based on the previously defined LSTM cell We have only one layer 'cell = rnn.MultiRNNCell ([LstmCell () for _ in range (NUM_LAYERS)]' to convert the stacked LSTM units into dynamic LSTM units' output, that can be updated during training _ = tf.nn.dynamic_rnn (cell, X, dtype=tf.float32) 'based on the predefined number of neurons per layer,' output = tf.reshape (output, [-1, The linear regression is calculated by the full join layer of the inactive function, and the data is compressed into a one-dimensional array structure 'predictions = tf.contrib.layers.fully_connected (output, 1, None) 'labels = tf.reshape (y, [-1]) predictions = tf.reshape (predictions, [-1])' defines the loss function. Here we define the optimizer parameters' train_op = tf.contrib.layers.optimize_loss' for the normal MSE 'loss = tf.losses.mean_squared_error (predictions, labels)' (loss,) Tf.contrib.framework.get_global_step (), optimizer='Adagrad', Learning_rate=0.6) 'returns predictive value, loss function and optimizer' return predictions, loss, train_op",
    "cell = tf.nn.rnn_cell.LSTMCell(num_units=64, state_is_tuple=True)": "单元格=tf.nn.rnn_cell.LSTMCell(num_unit=64，state_is_tuple=True)",
    "outputs, last_states = tf.nn.dynamic_rnn(\n    cell=cell,\n    dtype=tf.float64,\n    sequence_length=X_lengths,\n    inputs=X)": "输出，最后状态=tf.nn.DynamicRNN(cell=cell，dtype=tf.Float 64，Sequence_Length=X_Length，Input=X)",
    "discriminator": "n. 辨别者，鉴别器;",
    "    def lstm_cell():\n      return tf.contrib.rnn.BasicLSTMCell(\n          hparams.dis_rnn_size,\n          forget_bias=0.0,\n          state_is_tuple=True,\n          reuse=reuse)": "def LSTM_cell()：返回tf.contrib.rn.BasicLSTMCell(hpars.dis_rnn_size，遗忘_偏差=0.0，state_is_tuple=True，重用=重用)",
    "BasicLSTMCell": "BasicLSTMCell",
    "tf.nn.rnn_cell.LSTMCell": "tf.nn.rnn_cell.",
    "def lstm_cell(cell_size, keep_prob, num_proj):\n  return tf.contrib.rnn.DropoutWrapper(\n      tf.contrib.rnn.LSTMCell(cell_size, num_proj=min(cell_size, num_proj)),\n      output_keep_prob=keep_prob)": "def lstm_cell(cell_size，备存_prob，num_proj)：返回tf.contrib.rnn.DropoutWrapper(tf.contrib.rnn.LSTMCell(cell_size，num_proj=min(cell_size，num_proj)，输出_备_prob=备存_prob)",
    "def multi_lstm_cell(cell_sizes, keep_prob, num_proj):\n  return tf.contrib.rnn.MultiRNNCell([lstm_cell(cell_size, keep_prob, num_proj)\n                                      for cell_size in cell_sizes])": "DEF_LSTM_cell(单元大小，保持_prob，num_proj)：返回单元大小为单元大小的tf.contrib.rnn.MultiRNNCell([lstm_cell(cell_size，保持_prob，num_proj)",
    "Semi-Supervised Sequence Modeling with Cross-View Training": "基于交叉视点训练的半监督序列建模",
    "Unsupervised representation learning algorithms such as word2vec and ELMo improve the accuracy of many supervised NLP models, mainly because they can take advantage of large amounts of unlabeled text. However, the supervised models only learn from task-specific labeled data during the main training phase. We therefore propose Cross-View Training (CVT), a semi-supervised learning algorithm that improves the representations of a Bi-LSTM sentence encoder using a mix of labeled and unlabeled data. On labeled examples, standard supervised learning is used. On unlabeled examples, CVT teaches auxiliary prediction modules that see restricted views of the input (e.g., only part of a sentence) to match the predictions of the full model seeing the whole input. Since the auxiliary modules and the full model share intermediate representations, this in turn improves the full model. Moreover, we show that CVT is particularly effective when combined with multi-task learning. We evaluate CVT on five sequence tagging tasks, machine translation, and dependency parsing, achieving state-of-the-art results.": "无监督表示学习算法如Word2vec和Elmo提高了许多有监督的NLP模型的准确性，主要是因为它们可以利用大量的未标记文本。然而，在主训练阶段，监督模型仅从特定任务的标记数据中学习.因此，我们提出了交叉视图训练(CVT)，这是一种半监督学习算法，它使用标记和未标记数据的混合改进了Bi-LSTM语句编码器的表示。在有标记的例子上，使用标准的监督学习。在未标注的示例中，CVT教授辅助预测模块，这些模块可以查看输入的受限视图(例如，句子的一部分)，以与看到整个输入的完整模型的预测相匹配。由于辅助模块和完整模型共享中间表示，这反过来又改进了整个模型。此外，我们还表明，CVT与多任务学习相结合尤其有效.我们评估CVT的五个序列标记任务，机器翻译和依赖分析，以取得最先进的结果.",
    "multitask_model": "多任务模型",
    "Model": "n. 模型;模特儿;模式;典型;\nvt.& vi. 做模特儿;\nvt. 模仿;制作模型，塑造;将…做成模型;\nadj. 典型的，模范的;",
    "multi_lstm_cell": "多LSTM单元",
    "cell_sizes": "细胞大小",
    "keep_prob": "保持原点",
    "    def multi_lstm_cell(cell_sizes, keep_prob, num_proj):\n": "def多个lstm_cell(单元大小，保持_prob，num_proj)：",
    "num_proj": "num proj",
    "nn.rnn_cell": "nn.rnn细胞",
    "nn.rnn_cel": "nn.rnn细胞",
    "nn.rnn_cell.LSTMCell": "nn.rnn_cell.LSTMCell",
    "If None, no projection is performed.": "如果没有，则不执行投影。",
    "DropoutWrapper": "DropoutWrappe",
    "dynamic_rnn": "动态RNN",
    "Creates a recurrent neural network specified by RNNCell cell.\n\nPerforms fully dynamic unrolling of inputs.": "创建由RNNCell细胞指定的递归神经网络。执行输入的完全动态展开。",
    "Performs": "v. 执行( perform的第三人称单数 );履行;表演;扮演;",
    "X_lengths": "X长度",
    "\n        sequence_length=X_lengths,": "序列长度=X长度，",
    "\n    cell = rnn.MultiRNNCell([LstmCell() for _ in range(NUM_LAYERS)])": "cell=rnn.MultiRNNCell([LstmCell()for_in range(NUM_Layers)])",
    "\n    '''将已经堆叠起的LSTM单元转化成动态的可在训练过程中更新的LSTM单元'''": "'convert an already stacked LSTM unit into a dynamic LSTM unit that can be updated during training.'",
    "\n": "",
    "\n    output, _ = tf.nn.dynamic_rnn(cell, X, dtype=tf.float32)\n": "输出，_=tf.nn.DynamicRNN(单元格，X，dtype=tf.Float 32)",
    "HIDDEN_SIZE": "HIDDEN_SIZE",
    "import numpy as np\nimport tensorflow as tf\nimport matplotlib as mpl\nfrom matplotlib import pyplot as plt\nfrom tensorflow.contrib.learn.python.learn.estimators.estimator import SKCompat\n\n# TensorFlow的高层封装TFLearn\nlearn = tf.contrib.learn\n\n# 神经网络参数\nHIDDEN_SIZE = 30  # LSTM隐藏节点个数\nNUM_LAYERS = 2  # LSTM层数\nTIMESTEPS = 10  # 循环神经网络截断长度\nBATCH_SIZE = 32  # batch大小\n\n# 数据参数\nTRAINING_STEPS = 3000  # 训练轮数\nTRAINING_EXAMPLES = 10000  # 训练数据个数\nTESTING_EXAMPLES = 1000  # 测试数据个数\nSAMPLE_GAP = 0.01  # 采样间隔\n\n\ndef generate_data(seq):\n    # 序列的第i项和后面的TIMESTEPS-1项合在一起作为输入，第i+TIMESTEPS项作为输出\n    X = []\n    y = []\n    for i in range(len(seq) - TIMESTEPS - 1):\n        X.append([seq[i:i + TIMESTEPS]])\n        y.append([seq[i + TIMESTEPS]])\n    return np.array(X, dtype=np.float32), np.array(y, dtype=np.float32)\n\n\n# LSTM结构单元\ndef LstmCell():\n    lstm_cell = tf.contrib.rnn.BasicLSTMCell(HIDDEN_SIZE)\n    return lstm_cell\n\n\ndef lstm_model(X, y):\n    # 使用多层LSTM，不能用lstm_cell*NUM_LAYERS的方法，会导致LSTM的tensor名字都一样\n    cell = tf.contrib.rnn.MultiRNNCell([LstmCell() for _ in range(NUM_LAYERS)])\n\n    # 将多层LSTM结构连接成RNN网络并计算前向传播结果\n    output, _ = tf.nn.dynamic_rnn(cell, X, dtype=tf.float32)\n    output = tf.reshape(output, [-1, HIDDEN_SIZE])\n\n    # 通过无激活函数的全联接层计算线性回归，并将数据压缩成一维数组的结构\n    predictions = tf.contrib.layers.fully_connected(output, 1, None)\n\n    # 将predictions和labels调整为统一的shape\n    y = tf.reshape(y, [-1])\n    predictions = tf.reshape(predictions, [-1])\n\n    # 计算损失值\n    loss = tf.losses.mean_squared_error(predictions, y)\n\n    # 创建模型优化器并得到优化步骤\n    train_op = tf.contrib.layers.optimize_loss(\n        loss,\n        tf.train.get_global_step(),\n        optimizer='Adagrad',\n        learning_rate=0.1)\n\n    return predictions, loss, train_op\n\n\n# 用sin生成训练和测试数据集\ntest_start = TRAINING_EXAMPLES * SAMPLE_GAP\ntest_end = (TRAINING_EXAMPLES + TESTING_EXAMPLES) * SAMPLE_GAP\ntrain_X, train_y = generate_data(\n    np.sin(np.linspace(0, test_start, TRAINING_EXAMPLES, dtype=np.float32)))\ntest_X, test_y = generate_data(\n    np.sin(\n        np.linspace(test_start, test_end, TESTING_EXAMPLES, dtype=np.float32)))\n\n# 建立深层循环网络模型\nregressor = SKCompat(learn.Estimator(model_fn=lstm_model, model_dir='model/'))\n\n# 调用fit函数训练模型\nregressor.fit(train_X, train_y, batch_size=BATCH_SIZE, steps=TRAINING_STEPS)\n\n# 使用训练好的模型对测试集进行预测\npredicted = [[pred] for pred in regressor.predict(test_X)]\n\n# 计算rmse作为评价指标\nrmse = np.sqrt(((predicted - test_y)**2).mean(axis=0))\nprint('Mean Square Error is: %f' % (rmse[0]))\n\n# 对预测曲线绘图，并存储到sin.jpg\nfig = plt.figure()\nplot_predicted, = plt.plot(predicted, label='predicted')\nplot_test, = plt.plot(test_y, label='real_sin')\nplt.legend([plot_predicted, plot_test], ['predicted', 'real_sin'])\nplt.show()\r\n--------------------- \r\n作者：widiot8023 \r\n来源：CSDN \r\n原文：https://blog.csdn.net/white_idiot/article/details/78882856 \r\n版权声明：本文为博主原创文章，转载请附上博文链接！": "",
    "\n--------------------- \n作者：widiot8023 \n来源：CSDN \n原文：https://blog.csdn.net/white_idiot/article/details/78882856 \n版权声明：本文为博主原创文章，转载请附上博文链接！": "Author: widiot8023 Source: CSDN original: https://blog.csdn.net/white_idiot/article/details/78882856 copyright notice: this article is original article by blogger, Please attach a link to the blog post!",
    "learn": "vt.& vi. 学习，学会;习得;得知;记住;\nvt. 记住;学习;得知;认识到;\nvi. 学习;获知;",
    "tf.estimator.": "估计量",
    "estimator": "n. 评价者，评估特有者;估测者;",
    "from tensorflow. import Estimator": "来自坦索弗洛。进口估计",
    "deprecation": "n. 强烈不赞成，祈免，反对;",
    "tensorflow.contrib.learn.python": "tensorflow.contrib.learn.python",
    "model_fn=lstm_model": "模型FN=LSTM模型",
    ", model_dir='model/'": "，模型_dir=‘模型/’",
    "def _build_model(features, labels, mode, params):\n        # 1. Configure the model via TensorFlow operations\n        # Connect the first hidden layer to input layer (features) with relu activation\n        y = tf.contrib.layers.fully_connected(features, num_outputs=64, activation_fn=tf.nn.relu,\n                                              weights_initializer=tf.contrib.layers.xavier_initializer())\n        y = tf.contrib.layers.fully_connected(y, num_outputs=64, activation_fn=tf.nn.relu,\n                                              weights_initializer=tf.contrib.layers.xavier_initializer())\n        y = tf.contrib.layers.fully_connected(y, num_outputs=1, activation_fn=tf.nn.sigmoid,\n                                              weights_initializer=tf.contrib.layers.xavier_initializer())\n\n        predictions = y\n\n        # 2. Define the loss function for training/evaluation\n        if mode == tf.estimator.ModeKeys.TRAIN or mode == tf.estimator.ModeKeys.EVAL:\n            loss = tf.reduce_mean((predictions - labels) ** 2)\n        else:\n            loss = None\n\n        if mode != tf.estimator.ModeKeys.PREDICT:\n            eval_metric_ops = {\n                \"rmse\": tf.metrics.root_mean_squared_error(tf.cast(labels, tf.float32), predictions),\n                \"accuracy\": tf.metrics.accuracy(tf.cast(labels, tf.float32), predictions),\n                \"precision\": tf.metrics.precision(tf.cast(labels, tf.float32), predictions)\n            }\n        else:\n            eval_metric_ops = None\n\n        # 3. Define the training operation/optimizer\n        if mode == tf.estimator.ModeKeys.TRAIN:\n            train_op = tf.contrib.layers.optimize_loss(\n                loss=loss,\n                global_step=tf.contrib.framework.get_global_step(),\n                learning_rate=0.001,\n                optimizer=\"Adam\")\n        else:\n            train_op = None\n\n        if mode == tf.estimator.ModeKeys.PREDICT:\n            predictions_dict = {\"pred\": predictions}\n        else:\n            predictions_dict = None\n\n        # 5. Return predictions/loss/train_op/eval_metric_ops in ModelFnOps object\n        return tf.estimator.EstimatorSpec(mode=mode,\n                                          predictions=predictions_dict,\n                                          loss=loss,\n                                          train_op=train_op,\n                                          eval_metric_ops=eval_metric_ops)\n    return _build_model": "def_build_model(特性、标签、模式、参数)：#1。通过TensorFlow操作配置模型#将第一个隐藏层连接到输入层(特性)，使用relu激活y=tf.contrib.layers.完全连接(特性，num_Output=64，activation_fn=tf.n.relu，完全连接(y，num_weights_initializer=tf.contrib.layers.xavier_initializer())=64，activation_fn=tf.n.relu，weights_initializer=tf.contrib.layers.xavier_initializer())y=tf.contrib.layers.完全连接(y，num_weights_initializer=tf.contrib.layers.xavier_initializer())=1，activation_fn=tf.n.Sigmoid，weights_initializer=tf.contrib.layers.xavier_initializer())预测=y#2。定义用于培训/评估的损失函数，如果mode=tf.估值器.modeKeys.TRAIN或mode=tf.估值器.ModeKeys.EVAL：LOAD=tf.ReduceMeans(预测标签)*2)无if模式！=tf.估值器.ModeKeys.PREDICT：Eval_MEGEROPS={“rmse”：tf.metrics.root_mean_squared_error(tf.cast(labels，“预测”、“准确性”、“精度”、)其他：Eval_METRICOPS=NONE#3。定义培训操作/优化器，如果模式=tf.估值器.ModeKeys.TRAIN：TRAIN_OP=tf.contrib.layers.最优化_LOAD(损失=损失，global_step=tf.contrib.framework.get_global_step()，)学习率=0.001，优化器=“adam”)，否则：TRANOP=NONE如果模式=tf.估值器.modeKeys.PREDICT：预测_dict={“pred”：预测}or：预测数字=无#5。返回ModelFnOps对象中的预测/丢失/TRANS_OP/val_METRIAL_OPS对象返回tf.估值器。损失=损失，列车运行=列车运行，Eval_米制_OPS=val_米制_OPS)返回_Build_模型",
    "SKCompat deprecation": "SKCOMPAT折旧",
    "\ntf.estimator.Estimator(model_fn=lambda :lstm_model, model_dir='model/')": "估计值(model_fn=lambda：lstm_model，model_dir=‘model/’)",
    "tf.contrib.learn": "学习",
    "learn.Estimator(model_fn=lstm_model, model_dir='model/')": "估计(model_fn=lstm_model，model_dir=‘model/’)",
    "estimator.fit": "估值器",
    "import numpy as np\nimport tensorflow as tf\nfrom tensorflow.examples.tutorials.mnist import input_data\n\ntf.logging.set_verbosity(tf.logging.INFO)\n\ndef lenet(x, is_training):\n    x = tf.reshape(x, shape=[-1, 28, 28, 1])\n\n    conv1 = tf.layers.conv2d(x, 32, 5, activation=tf.nn.relu)\n    conv1 = tf.layers.max_pooling2d(conv1, 2, 2)\n\n    conv2 = tf.layers.conv2d(conv1, 64, 3, activation=tf.nn.relu)\n    conv2 = tf.layers.max_pooling2d(conv2, 2, 2)\n\n    fc1 = tf.contrib.layers.flatten(conv2)\n    fc1 = tf.layers.dense(fc1, 1024)\n    fc1 = tf.layers.dropout(fc1, rate=0.4, training=is_training)\n    return tf.layers.dense(fc1, 10)\n\ndef model_fn(features, labels, mode, params):\n    predict = lenet(\n        features[\"image\"], mode == tf.estimator.ModeKeys.TRAIN)\n\n    if mode == tf.estimator.ModeKeys.PREDICT:\n        return tf.estimator.EstimatorSpec(\n            mode=mode,\n            predictions={\"result\": tf.argmax(predict, 1)})\n\n    loss = tf.reduce_mean(\n        tf.nn.sparse_softmax_cross_entropy_with_logits(\n           logits=predict, labels=labels))\n\n    optimizer = tf.train.GradientDescentOptimizer(\n        learning_rate=params[\"learning_rate\"])\n\n    train_op = optimizer.minimize(\n        loss=loss, global_step=tf.train.get_global_step())\n\n    eval_metric_ops = {\n        \"accuracy\": tf.metrics.accuracy(\n            tf.argmax(predict, 1), labels)\n    }\n\n    return tf.estimator.EstimatorSpec(\n        mode=mode,\n        loss=loss,\n        train_op=train_op,\n        eval_metric_ops=eval_metric_ops)\n\nmnist = input_data.read_data_sets(\"../../datasets/MNIST_data\", one_hot=False)\n\nmodel_params = {\"learning_rate\": 0.01}\nestimator = tf.estimator.Estimator(model_fn=model_fn, params=model_params)\n  \ntrain_input_fn = tf.estimator.inputs.numpy_input_fn(\n      x={\"image\": mnist.train.images},\n      y=mnist.train.labels.astype(np.int32),\n      num_epochs=None,\n      batch_size=128,\n      shuffle=True)\n\nestimator.train(input_fn=train_input_fn, steps=30000)": "将numpy作为NP导入TensorFlow作为TF从tensorflow.examples.tutorials.mnist导入INPUT_Data tf.logging.set_vility(tf.logging.INFO)def lenet(x，is_培训)：x=tf.repepe(x，form=[-1，28，28，28，1]conc 1=tf.layers.concon2d(x，32，5，activation=tf.nn.relu)conc 1=tf.layers.max_cluing2d(conf 1，2，2)con2=tf.layers.con2d(conf 1，64，3，激活=tf.nn.relu)con2=tf.layers.max_cluding 2d(conf 2，2，2)fc1=tf.contrib.layers.flatten(Conv 2)fc1=tf.layers.稠密(FC1，1024)FC1=tf.layers.diout(FC1，速率=0.4，返回tf.layers.稠密(FC1，10)def模型_fn(特征、标签、模式、参数)：Predict=lenet(特征[“图像”]，模式=tf.估值器.modeKeys.TRAIN)如果模式=tf.估值器.ModeKeys.PREDICT：返回tf.估值器.EstimatorSpec(模式=模式，预测={“结果”：tf.argmax(预测，(1)损失=tf.down_均(tf.nn.sparse_softmax_cross_entropy_with_logits(逻辑=预测，(标签=标签)优化器=tf.tram.GRadientDescentizer(学习速率=Params[“学习速率”])TRANOP=优化器.最小化(损失=损失，全局_STEP=tf.Tra.get_global_STEP()val_METRIAL_OPS={“精度”：tf.emeics.精确度(tf.argmax(预测，1)，标签)}返回tf.估测器.EstimatorSpec(模式=模式，损失=损失，火车_OP=火车_OP，val_米制_OP=val_米制_OP)mnist=input_data.read_data_sets(“../datasets/MNIST_data”，Estimator(model_fn=model_fn，params=model_params)TRAN_INPUT_fn=tf.估值器.inputs.numpy_put_fn(x={“映像”：mnist.tra.Image}，y=mnist.tra.labels.astype(np.int 32)，num_EIRCHS=NONE，Batch_SIZE=128，STOY=True)估计器(INPUT_FN=TRANS_INPUT_FN，STEP=30000)",
    "# 使用多层LSTM，不能用lstm_cell*NUM_LAYERS的方法，会导致LSTM的tensor名字都一样\n    cell = tf.contrib.rnn.MultiRNNCell([LstmCell() for _ in range(NUM_LAYERS)])\n\n    # 将多层LSTM结构连接成RNN网络并计算前向传播结果\n    output, _ = tf.nn.dynamic_rnn(cell, X, dtype=tf.float32)": "# using multilayer LSTM, does not work with lstm_cell*NUM_LAYERS, Causes LSTM to have the same tensor name as cell = tf.contrib.rnn.MultiRNNCell ([LstmCell () for _ in range (NUM_LAYERS]) # connects the multilayer LSTM structure into a RNN network and computes the forward propagation result output, _ = tf.nn.dynamic_rnn (cell, X, dtype=tf.float32)",
    "stulstm": "stulstm",
    "def model_fn(features, labels, mode, params):\n    predict = lenet(\n        features[\"image\"], mode == tf.estimator.ModeKeys.TRAIN)\n\n    if mode == tf.estimator.ModeKeys.PREDICT:\n        return tf.estimator.EstimatorSpec(\n            mode=mode,\n            predictions={\"result\": tf.argmax(predict, 1)})\n\n    loss = tf.reduce_mean(\n        tf.nn.sparse_softmax_cross_entropy_with_logits(\n           logits=predict, labels=labels))\n\n    optimizer = tf.train.GradientDescentOptimizer(\n        learning_rate=params[\"learning_rate\"])\n\n    train_op = optimizer.minimize(\n        loss=loss, global_step=tf.train.get_global_step())\n\n    eval_metric_ops = {\n        \"accuracy\": tf.metrics.accuracy(\n            tf.argmax(predict, 1), labels)\n    }\n\n    return tf.estimator.EstimatorSpec(\n        mode=mode,\n        loss=loss,\n        train_op=train_op,\n        eval_metric_ops=eval_metric_ops)": "def模型_fn(特征、标签、模式、参数)：预测=连载(特征[“图像”]，模式=tf.估值器.ModeKeys.TRAIN)如果模式=tf.估值器.ModeKeys.PREDICT：返回tf.估值器.EstimatorSpec(模式=模式，预测={“结果”：tf.argmax(预测，1})损失=tf.cr_均(tf.nn.sparse_softmax_cross_entropy_with_logits(逻辑=预测，(标签=标签)优化器=tf.tram.GRadientDescentizer(学习速率=Params[“学习速率”])TRANOP=优化器.最小化(损失=损失，全局_STEP=tf.Tra.get_global_STEP()val_METRIAL_OPS={“精度”：tf.emeics.精确度(tf.argmax(预测，1)，标签)}返回tf.估测器.EstimatorSpec(模式=模式，(损失=损失，火车_OP=火车_OP，EVA_米制_OP=EVA_十进制_OPS)",
    "def lstm_model(X, y, is_training):\n    # 使用多层的LSTM结构。\n    cell = tf.nn.rnn_cell.MultiRNNCell([\n        tf.nn.rnn_cell.BasicLSTMCell(HIDDEN_SIZE) \n        for _ in range(NUM_LAYERS)])    \n\n    # 使用TensorFlow接口将多层的LSTM结构连接成RNN网络并计算其前向传播结果。\n    outputs, _ = tf.nn.dynamic_rnn(cell, X, dtype=tf.float32)\n    output = outputs[:, -1, :]\n\n    # 对LSTM网络的输出再做加一层全链接层并计算损失。注意这里默认的损失为平均\n    # 平方差损失函数。\n    predictions = tf.contrib.layers.fully_connected(\n        output, 1, activation_fn=None)\n    \n    # 只在训练时计算损失函数和优化步骤。测试时直接返回预测结果。\n    if not is_training:\n        return predictions, None, None\n        \n    # 计算损失函数。\n    loss = tf.losses.mean_squared_error(labels=y, predictions=predictions)\n\n    # 创建模型优化器并得到优化步骤。\n    train_op = tf.contrib.layers.optimize_loss(\n        loss, tf.train.get_global_step(),\n        optimizer=\"Adagrad\", learning_rate=0.1)\n    return predictions, loss, train_op": "Def lstm_model (X, y, is_training: # uses a multi-tier LSTM structure. Cell = tf.nn.rnn_cell.MultiRNNCell ([tf.nn.rnn_cell.BasicLSTMCell (HIDDEN_SIZE) for _ in range (NUM_LAYERS]) # using the TensorFlow interface to connect the multi-tier LSTM structure into RNN Network and calculate its forward propagation result. Outputs, _ = tf.nn.dynamic_rnn (cell, X, dtype=tf.float32) output = outputs [:, -1,:] # adds a layer of full link layer to the output of LSTM network and calculates the loss. Note that the default loss here is the average # square difference loss function. Predictions = tf.contrib.layers.fully_connected (output, 1, activation_fn=None) # only calculates loss functions and optimization steps during training. The predicted results are returned directly during the test. If not is_training: return predictions, None, None # calculates the loss function. Loss = tf.losses.mean_squared_error (labels=y, predictions=predictions) # creates the model optimizer and gets the optimization steps. Train_op = tf.contrib.layers.optimize_loss (loss, tf.train.get_global_step (), optimizer= \"Adagrad\"), learning_rate=0.1) return predictions, loss, train_op",
    "\ndef stulstm(X):\n    # 使用多层LSTM，不能用lstm_cell*NUM_LAYERS的方法，会导致LSTM的tensor名字都一样\n    cell = tf.contrib.rnn.MultiRNNCell([LstmCell() for _ in range(NUM_LAYERS)])\n\n    # 将多层LSTM结构连接成RNN网络并计算前向传播结果\n    output, _ = tf.nn.dynamic_rnn(cell, X, dtype=tf.float32)\n    return output": "Def stulstm (X): # uses a multilayer LSTM, that does not work with lstm_cell*NUM_LAYERS, Causes LSTM to have the same tensor name as cell = tf.contrib.rnn.MultiRNNCell ([LstmCell () for _ in range (NUM_LAYERS]) # connects the multilayer LSTM structure into a RNN network and computes the forward propagation result output, _ = tf.nn.dynamic_rnn (cell, X, dtype=tf.float32) return output",
    "tf.contrib.layers.fully_connected": "tf.contrib.layers.完全连接",
    "model_params": "模型傍线",
    ".numpy_input_fn(x=test_X, y=test_y, num_epochs=1, batch_size=128, shuffle=False)": ".numpy_input_fn(x=test_x，y=test_y，num_pochs=1，批处理_size=128，霉运=false)",
    "oss = tf.reduce_mean(tf.nn.sparse_softmax_cross_entropy_with_logits(logits=predict, labels=labels))": "=tf.reduce_mean(tf.nn.sparse_softmax_cross_entropy_with_logits(logits=predict，标签=标签)",
    "loss = tf.reduce_mean(tf.nn.sparse_softmax_cross_entropy_with_logits(logits=predict, labels=labels))": "损失=tf.reduce_mean(tf.nn.sparse_softmax_cross_entropy_with_logits(logits=predict，标签=标签)",
    "study/lstmstudy.py:71": "学习/学习：71",
    "ambiguous": "adj. 含糊的，不明确的;引起歧义的;有两种或多种意思的;模棱两可;",
    "consum['brush_time'": "画笔时间",
    "=consum['brush_time']": "‘刷子时间’=consum[‘画笔时间’]",
    "=": "=",
    "slots[slots['times'].between('21:00:00', '02:00:00')]": "插槽[‘时代’之间(‘21：00：00’，‘02：00：00’)]",
    "between": "prep. 在…之间;私下，暗中;在…中任择其一;来往于…之间;\nadv. 当中，中间;",
    "between_time('0:15', '0:45')": "时间之间(‘0：15’，‘0：45’)",
    "consum['brush_time']": "画笔时间",
    "between_time": "between_time",
    "'0:15', '0:45'": "'0:15', '0:45'",
    ".head()": ".head()",
    "brush_time": "刷时间",
    "['brush_time']": "[“刷时间”]",
    "consum.loc[consum.between_time('0:15', '0:45')]": "_time(‘0：15’，‘0：45’)",
    "consum['timeslot'] = '0'": "consum[‘Timeeslot’]=‘0’",
    "timeslot": "time slot时间空档",
    "datetime.time() + datetime.timedelta()": "datetime.time()datetime.timeDelta()",
    "+ datetime.timedelta(hours=1)": "datetime.timeDelta(小时=1)",
    "datetime.datetime.today()": "日期时间。今天()",
    "datetime.datetime.date()": "日期.日期()",
    ".today()": ".（在）今天()",
    "datetime.timedelta(hours=1)": "datetime.timeDelta(小时=1)",
    "tdelta": "delta希腊语字母表第四字母δ",
    "tdelta = ": "tdelta =",
    "sleep": "vi.& link-v. 睡，睡觉;\nvi. 睡，睡觉;睡眠状态;\nvt. 为…提供床位;提供住宿;以睡觉打发日子;\nn. 睡眠;",
    "上午": "forenoon",
    "midnight": "n. 子夜，午夜;漆黑;一段黑暗阴暗的时间;漏夜;",
    "6:45": "6:45",
    " include_end=True": "End=True",
    "11:55": "11:55",
    "2:30": "2:30",
    "5:55": "5:55",
    "consum.loc[consum.between_time('0:0', '6:45', include_end=False), 'timeslot'] = 'sleep'": "_time(‘0：0’，‘6：45’，include_end=false)，‘Timeeslot’]=‘睡眠’",
    "8:30": "8:30",
    "14:30": "14:30",
    "17:55": "17:55",
    "19:30": "19:30",
    "21:55": "21:55",
    "consum.loc[consum.between_time('21:55', '23:59', include_end=True), 'timeslot'] = 'sleep'": "间隔期(‘21：55’，‘23：59’，include_end=True)，‘Timeeslot’]=‘睡眠’",
    "consum.between_time('21:55', '23:59', include_end=True)": "消费时间(‘21：55’，‘23：59’，包括_End=True)",
    "consum.between_time('21:55', '0:0', include_end=False)": "_time(‘21：55’，‘0：0’，include_end=false)",
    "breakfast": "n. 早餐，早饭;早餐食物;\nvi. 吃早餐;\nvt. 供给…早餐;",
    "morning": "n. 早晨;上午;黎明;早期，初期;",
    "noon": "n. 正午，中午;全盛期，顶点;〈古〉午夜;晌午;",
    "afternoon": "n. 下午;后期，后部;",
    "dinner": "n. 正餐，主餐;宴会;晚餐;",
    "night": "n. 夜;晚上;（举行盛事的）夜晚;",
    "consum.loc[consum.between_time('21:55', '0:0', include_end=False), 'timeslot'] = 'sleep'": "_time(‘21：55’，‘0：0’，include_end=false)，‘Timeeslot’]=‘睡眠’",
    ".index": ".索引",
    "consum.index = consum.brush_time": "消耗品.索引=消耗品.刷子时间",
    "consum['brush_time'] = pd.to_datetime(consum.brush_time)": "consum[‘画笔_时间’]=pd.to_datetime(消耗品_时间)",
    "card_id": "卡片ID",
    "consum.drop(['brush_time'], axis=1, inplace=True)": "下拉([‘画笔_时间’]，轴=1，内置=真)",
    "\ndatetime.datetime.date()\ndatetime.time() + datetime.timedelta(hours=1)": "datetime.date()datetime.time()datetime.timeDelta(小时=1)",
    "consum_placemap['place0'] = consum_placemap.place.str.split('_').str.get(0)": "consum_placemap[‘place 0’]=consum_placemap.place.str.split(‘_’).str.get(0)",
    "map['place0']": "地图[‘place0’]",
    "consum_place": "Consum场所",
    "onsum.place": "安顿",
    "consum.place": "消费场所",
    ".value_counts()": ".value_count()",
    "consum.place.str": "消费.Place.",
    "train": "n. 火车;行列;一系列相关的事情; 拖裾;\nv. 训练;教育;培养;修整;",
    "consum.place.str.": "消费场所。",
    "沙河校区": "Shahe campus",
    " 'null',": "'null',",
    "后勤集团车队": "Logistics group fleet",
    "本大仙要上班，很忙，忙着在天猫购物，抢秒杀，收快递。所以在评价方面会比较偷懒，所以特地模仿别人写下这个模板。下面开始我的评语，首先表扬店家请的客服，因为从始至终他都没有打扰我，当然我也没有向他提问。购物这么多年，哪有时间去问这问那。直接买，不好就退货，好就留着。今天本大仙给了你好评就证明我认可了这个宝贝，如果不满意的话，本大仙会生气，那么后果很严重，你们也不会看到这个好评模板。—–本评语来自一位忙的没时间写评语的大仙。": "Ben Tai Sin to work, very busy, busy shopping in Tmall, snatch easily beat, collect express. So you will be lazy in evaluation, so imitate others to write down this template. Let me start with my comments, first of all, praise the customer service he invited, because he didn't bother me all the time, and I didn't ask him any questions. Shopping for so many years, there is no time to ask that. Buy it directly, return it if it's bad, and keep it. Today, when Ben Tai Sin gave you a favorable review, it proved that I recognized this baby. If he is not satisfied, Ben Tai Sin will be angry, then the consequences will be very serious. You won't see this praise template either.-this comment comes from a busy fairy who doesn't have time to write a comment.",
    "搞笑": "amuse",
    "notsplit = ['null',]": "NOTSCILE=[‘NULL’，]",
    "交通运输服务中心": "Transportation service centre",
    "图书馆": "library",
    "电子科技大学": "University of Electronic Science and Technology of China, UESTC",
    "硕士楼校园超市": "Master's Building Campus supermarket",
    "2FPOS": "aposapostrophe 省略符号，呼语",
    "沙河校区_": "Shahe Campus _",
    "校园超市": "Campus supermarket",
    "null": "adj. <术>零值的;等于零的;（协议） 无法律效力;失效的;",
    ".drop_duplicates()": ".DROP_CONDITES()",
    "consum[consum.place.str.startswith('null_图书馆')]": "Consum [consum.place.str.startswith ('null_ Library')]",
    "0051A713": "0051A713",
    "device_id": "device_id",
    "student_id": "学生身份证",
    "323567": "三十二万三千五百六十七",
    "consum[consum.student_id == '']": "consum，consum.",
    "consum = consum[consum.student_id != '']": "consum=consum[sum.sup_id！=‘]",
    " = consum[consum.student_id != '']": "=consum[sumer.sup_id！=‘]",
    "place": "n. 位;地方;职位;座位;\nvt. 放置;获名次;投资;评价;\nvi. 得名次;名列前茅;[美国英语][赛马]得第二名;准确把…推到预定地点;",
    "一食堂": "A canteen",
    "consum[consum.place.str.startswith('电子科技大学_')]": "Consum [consum.place.str.startswith ('University of Electronic Technology')]",
    "consum[consum.place.str.startswith('电子科技大学_')].drop_duplicates()": "Consum [consum.place.str.startswith ('University of Electronic Science and Technology _')]. Drop_duplicates ()",
    "'电子科技大学'": "University of Electronic Technology",
    "re.": "abbr. reference 参考，参见;regarding 关于;",
    "re.findall()": "re.findall()",
    "电子科技大学_(\\w{3})\\dFPOS(\\w+)": "University of Electronic Science and Technology _ (\\ w {3})\\ dFPOS (\\ w)",
    "startswith": "start with以…开始",
    "res = re.findall('电子科技大学_(\\w{3})\\dFPOS(\\w+)', x)[0]\n        print(res)\n        return res": "Res = re.findall ('University of Electronic Science and Technology _ (\\ w {3})\\ dFPOS (\\ w)', x) [0] print (res) return res",
    "(\\w{3})\\dFPOS": "(W{3})\\dFPOS",
    "elif x.startswith('null'):\n        res = re.findall('null_(\\w+)', x)[0]\n        print(res)": "Elifx.startswith(‘null’)：res=re.findall(‘null_(\\w)’，x)[0]print(Res)",
    "沙河图书馆": "Shahe library",
    "notsplit = ['null','电子科技大学']": "Notsplit = ['null',' University of Electronic Technology']",
    "consum.place.": "就位。",
    "_": "_",
    "re.findall('电子科技大学_(\\w{3})\\dFPOS(\\w+)', x)[0]": "Re.findall ('University of Electronic Technology _ (\\ w {3})\\ dFPOS (\\ w)', x) [0]",
    "电子科技大学_一食堂2FPOS68": "University of Electronic Science and Technology _ 1 cafeteria 2FPOS68",
    "[0][0]\n": "[0][0]",
    "[0]": "[0]",
    "res = x": "RES=x",
    "else:\n        res = x": "其他：res=x",
    "\n    ": "",
    "POS": " <美口>便壶，夜壶(po的复数);",
    "\n            print(res)": "打印(RES)",
    "\n    except Exception as e:\n        res = x\n        print(x)": "除e：res=x print(X)的例外情况外",
    "\n            return res": "返回区域",
    "if x.startswith('电子科技大学_'):\n            res = re.findall('电子科技大学_(\\w{3})\\dF(.+)', x)[0][0]": "If x.startswith ('University of Electronic Science and Technology _'): res = re.findall ('University of Electronic Science and Technology' _ (\\ w {3})\\ dF (. )', x) [0] [0]",
    "(.+)": "(.+)",
    "(.+)POS(.+)": "(.+)POS(.+)",
    "文印中心": "Text printing cente",
    "re.findall('电子科技大学_(.+)POS(.+)', x)": "Re.findall. ) POS (. ', x)",
    "findall": "fin wall翼缘墙",
    "'电子科技大学_(\\w{3})\\dF(.+)'": "'University of Electronic Science and Technology _ (\\ w {3})\\ dF (. )'",
    "'null_(\\w+)'": "‘NULL_(\\w)’",
    "'沙河校区_'": "'Shahe Campus _'",
    "'电子科技大学_(.+)POS(.+)'": "University of Electronic Science and Technology. ) POS (. )'",
    "(\\w+)": "(\\w+)",
    "'电子科技大学_(.+)POS.+',": "University of Electronic Science and Technology. POS. '",
    "收费": "collect fees",
    "re.findall": "重来",
    "'''\nnull\n\n# 沙河校区_ 沙河图书馆 只有1类\n\n'''": "'null # Shahe Campus _ Shahe Library only Class 1'",
    "\n# consum_place = consum.place.str.split('_').str.get(0)": "#consum_place=sum.place.str.split(‘_’).str.get(0)",
    "tong": "n. 钳;煤钳;堂;帮会;\nv. 用钳子钳起;",
    "general": "adj. 大致的;综合的;总的，全体的;普遍的;\nn. 上将;一般;一般原则;常规;",
    "res\n        res = x.split('_')[0]": "RES=x.split(‘_’)[0]",
    "\n                return res\n        res = x.split('_')[0]": "返回res=x.split(‘_’)[0]",
    "+": "+",
    "'(\\w+)_.+'": "'(\\w+)_.+'",
    "硕士16#楼_硕士16#楼6F淋041": "Master 16# Building _ Master 16# Building 6F shower 041",
    "re.match(p, x)": "重匹配(p，x)",
    "print(x)": "打印(X)",
    "\n    except Exception as e:\n        print(x)": "除e例外：打印(X)",
    "\n    try:": "atry顶浪",
    "more than meets the eye": " 事情并不象初看到的那样简单;",
    "It was decided at some point long ago getting the length of something should be a function and not a method code, reasoning that len(a)'s meaning would be clear to beginners but a.len() would not be as clear. When Python started __len__ didn't even exist and len was a special thing that worked with a few types of objects. Whether or not the situation this leaves us makes total sense, it's here to stay.": "早在很久以前就决定了，获取某物的长度应该是一个函数，而不是一个方法代码，它推理说，连(A)的含义对初学者来说是清楚的，但a.len()就不那么清楚了。当Python启动时，连_都不存在，而len是一种特殊的东西，可以处理几种类型的对象。不管这种情况是否能让我们完全理解，它将留在这里。",
    "\n61\ndown vote\nIt's often the case that the \"typical\" behavior of a built-in or operator is to call (with different and nicer syntax) suitable magic methods (ones with names like __whatever__) on the objects involved. Often the built-in or operator has \"added value\" (it's able to take different paths depending on the objects involved) -- in the case of len vs __len__, it's just a bit of sanity checking on the built-in that is missing from the magic method:": "61否决投票-通常情况是，内置或运算符的“典型”行为是在所涉及的对象上调用(使用不同和更好的语法)适当的魔术方法(那些方法的名称为_)。通常，内置或运算符具有“附加价值”(它能够根据所涉及的对象采取不同的路径)-对于len VS_len_，它只是对内置的内容进行了一点理智检查-这是魔术方法中缺少的：",
    "It's often the case that the \"typical\" behavior of a built-in or operator is to call (with different and nicer syntax) suitable magic methods (ones with names like __whatever__) on the objects involved. Often the built-in or operator has \"added value\" (it's able to take different paths depending on the objects involved) -- in the case of len vs __len__, it's just a bit of sanity checking on the built-in that is missing from the magic method:": "通常情况下，内置或运算符的“典型”行为是在所涉及的对象上调用(使用不同和更好的语法)适当的魔术方法(那些方法的名称为_)。通常，内置或运算符具有“附加价值”(它能够根据所涉及的对象采取不同的路径)-对于len VS_len_，它只是对内置的内容进行了一点理智检查-这是魔术方法中缺少的：",
    "When you see a call to the len built-in, you're sure that, if the program continues after that rather than raising an exception, the call has returned an integer, non-negative, and less than 2**31 -- when you see a call to xxx.__len__(), you have no certainty (except that the code's author is either unfamiliar with Python or up to no good;-).": "当您看到对len内建的调用时，您可以确定，如果程序在此之后继续运行，而不是引发异常，则调用将返回一个整数、非负和小于2*31-当您看到对xxx._len_()的调用时，您无法确定(除了代码的作者对Python不熟悉或不太好)；)。",
    "Other built-ins provide even more added value beyond simple sanity checks and readability. By uniformly designing all of Python to work via calls to builtins and use of operators, never through calls to magic methods, programmers are spared from the burden of remembering which case is which. (Sometimes an error slips in: until 2.5, you had to call foo.next() -- in 2.6, while that still works for backwards compatibility, you should call next(foo), and in 3.*, the magic method is correctly named __next__ instead of the \"oops-ey\" next!-).\n\n": "其他内置功能除了简单的理智检查和可读性之外，还提供了更多的附加价值。通过统一设计所有Python，通过调用Builtins和使用操作符来工作，而不是通过调用神奇的方法，程序员可以免去记住哪一种情况是哪一种情况的负担。(有时会出现错误：在2.5之前，您必须调用foo.Next()-在2.6中，虽然这对于向后兼容性仍然有效，但是您应该调用Next(Foo)，在3.*中，魔术方法被正确命名为_Next_，而不是“OOPS-ey”Next！-)。",
    "So the general rule should be to never call a magic method directly (but always indirectly through a built-in) unless you know exactly why you need to do that (e.g., when you're overriding such a method in a subclass, if the subclass needs to defer to the superclass that must be done through explicit call to the magic method).": "因此，一般规则应该是永远不要直接调用魔术方法(但总是通过内置的方法间接调用)，除非您确切地知道需要这样做的原因(例如，当您在子类中重写这样的方法时，如果子类需要遵从必须通过显式调用魔术方法完成的超类)。",
    "command 'markdown.extension.togglePreviewToSide' not found": "命令‘markdown.扩展名.togglePreviewToSide’未找到",
    "sudo vi /etc/NetworkManager/NetworkManager.conf": "Sudo vi/etc/NetworkManager/NetworkManager.conf",
    "chkconfig dnsmasq off": "chkconfig dnsmasq关闭",
    "Markdown Preview Enhanced": "减价预览增强",
    "Preview": "n. 试映，预演;预告片;象征，预示;\nvt. 预映;预先观看;概述;扼要介绍;",
    "Markdown Preview Enhanced\n": "减价预览增强",
    "markdown.showPreviewToSide": "Markdown.Show PreviewToSide",
    "ctrl+shift+v": "Ctrl移位v",
    "-markdown.showPreview": "-Markdown.展示预览",
    "markdown ": "n. 减价;",
    "nameserver 127.0.1.1": "nameserver 127.0.1.1",
    "dnsmasq": "dnsmasq",
    "sanity": "n. 神志正常;心智健康;头脑清楚;通情达理;",
    "This has nothing to do with the above, which is about builtin len. You just defined your len function, that can of course return whatever you want. OP spoke about builtin len, which calls __len__ special method (not function) on the object under consideration": "这与上面的内容无关，这是关于内建的。您刚刚定义了len函数，当然，它可以返回您想要的任何内容。OP谈到了内置Len，它对正在考虑的对象调用_len_专用方法(而不是函数)。",
    "consum.drop(['place'], axis=1, inplace=True)": "下拉([‘Place’]，轴=1，inplace=True)",
    "consum=consum.reset_index(drop=True)\nconsum.drop(['brush_time'], axis=1, inplace=True)": "consum=消耗品.重置指数(DROP=True)消耗.DROP([‘画笔时间’]，轴=1，Inplace=True)",
    "import re\n'''\nnull\n\n# 沙河校区_ 沙河图书馆 只有1类\n\n'''\npattern = [\n    '电子科技大学_(\\w{3})\\dF.+',\n    'null_(\\w+)',\n    '沙河校区_(\\w+)',\n    '电子科技大学_(.+)POS.+',\n    '电子科技大学_(.+)收费.+',\n    '(.+)_.+'  # general\n]\ndef splitplace(x):\n    res = x\n    for p in pattern:\n        if re.match(p, x):\n            res = re.findall(p, x)[0]\n            return res\n    if x:\n        print(x)\n    return res\nconsum['place0'] = consum.place.apply(splitplace)\nconsum.drop(['place'], axis=1, inplace=True)": "The import re 'null # Shahe campus _ Shahe library has only one' pattern = 'University of Electronic Science and Technology _ (\\ dF.)\\ 'null_ (\\ w)', 'Shahe Campus\\ w', 'University of Electronic Science and Technology'. POS. University of Electronic Science and Technology ) fees. ','(1). ) '# general] def splitplace (x): res = x for p in pattern: if re.match (p, x): res = re.findall (p, X) [0] return res if x: print (x) return res consum ['place0'] = consum.place.apply (splitplace) consum.drop ([' place'], axis=1, inplace=True)",
    "m": "n. 英语字母表的第13个字母;",
    "campus": "n. 校园;校区;",
    "2674186.9499999997": "二百六十七万四千一百八十六点九四九九九九九九九七",
    "2674186": "二百六十七万四千一百八十六",
    "-2674186*2": "-2674186*2",
    "-2674186": "负二百六十七万四千一百八十六",
    "consum.iloc[-2674186*2:-2674186, :].to_csv('../data/tf_val.csv', index=False)": "-2674186*2：-2674186，].to_csv(‘./data/tf_val.csv’，index=false)",
    "-2674186*2:": "-2674186*2:",
    ", header=False": "，Header=false",
    "Brian Culbertson": "布莱恩·库伯森",
    "FIFTEEN_PCT": "FIFTEEN_PCT",
    "df": " dead freight 空舱费;",
    "for col in consum.columns:\n#     c = consum[col].drop_duplicates().count()\n#     print('col name: {} - {}'.format(col, c))\n    print(consum[col].isna().sum())": "c=consum[coll].头重复项().count()#print(‘colname：{}’格式(coll，c)打印(consum[coll].isna().sum()",
    "\ndf[categorical_features] = df[categorical_features].fillna('')\ndf[continus_features] = df[continus_features].fillna(-1)": "df[分类_特征]=df[分类_特征]。填充NA(‘)df[连续特征]=df[连续特征]。填充NA(-1)",
    "\n    # # use hash\n    # for feat in categorical_features:\n    #     if feat in embedding_features:\n    #         f = tf.feature_column.categorical_column_with_hash_bucket(feat, hash_bucket_size=_HASH_BUCKET_SIZE)\n    #     else:\n    #         f = tf.feature_column.categorical_column_with_vocabulary_list(feat, df[feat].unique())\n    #     cate_columns.append(f)": "#在分类_特性中使用散列#进行专长：#if feat in Embedded_Feature：#f=tf.feature_column.categorical_column_with_hash_bucket(feat，散列_桶_size=_散列_桶_size)#etc：#f=tf.feature_column.categorical_column_with_vocabulary_list(feat，df[feat].唯一()#cate_Columns.append(F)",
    "\n    # # no hash\n    # fullVisitorId": "#不散列#完全VisitorId",
    "\n    onehot_columns = []": "一个热柱=[]",
    "\n    crossed_cobhlumns = []": "交叉Cobhlumns=[]",
    "TOTAL_NUM": "总NUM",
    "\n_TRAIN_ID_SIZE = 714167": "列车ID尺寸=714167",
    "_TRAIN_ID_SIZE": "列车ID尺寸",
    "_TRAIN": "火车",
    "\n    embedding_features = []": "嵌入特征=[]",
    "\n    cate_embedding_columns = []\n    cate_no_embedding_columns = []": "凯特嵌入列=[]凯特_NO_嵌入列=[]",
    "\n    for i, feat1 in enumerate(categorical_features):\n        for feat2 in categorical_features[i + 1:]:\n            if feat1 != feat2:\n                f = tf.feature_column.crossed_column([feat1, feat2], hash_bucket_size=_HASH_BUCKET_SIZE)\n                crossed_columns.append(f)": "对于i，枚举中的特征1(分类_特征)：对于分类_特征中的特征2[I_1：]：如果特征1！=功能2：F=tf.Feature_Column.Cross_Column([Fee 1，Fee 2]，哈希_桶_大小=_哈希_桶_大小)交叉_列.追加(F)",
    "\n    wide_columns = crossed_columns + [id_column]": "宽列=交叉列[id_列]",
    "def lstm_model(X, y, mode):\n    # 使用多层的LSTM结构\n    cell = tf.nn.rnn_cell.MultiRNNCell([\n        tf.nn.rnn_cell.LSTMCell(HIDDEN_SIZE)\n        for _ in range(NUM_LAYERS)])\n    # 使用TensorFlow接口将多层的LSTM结构连接成RNN网络并计算其前向传播结果\n    outputs, _ = tf.nn.dynamic_rnn(cell, X, dtype=tf.float32)\n    output = outputs[:, -1, :]\n    # 对LSTM网络的输出再做加一层全链接层并计算损失，注意这里默认的损失为平均\n    # 平方差损失函数\n    predictions = tf.contrib.layers.fully_connected(output, 1, activation_fn=None)\n    if mode == tf.estimator.ModeKeys.PREDICT:\n        return predictions, None, None\n    # 计算损失函数\n    loss = tf.losses.mean_squared_error(labels=y, predictions=predictions)\n    if mode == tf.estimator.ModeKeys.EVAL:\n        return predictions, loss, None\n    # 创建模型优化器并得到优化步骤\n    train_op = tf.contrib.layers.optimize_loss(loss, tf.train.get_global_step(), optimizer=\"Adagrad\", learning_rate=0.1)\n    # 只在训练时计算损失函数和优化步骤，测试时直接返回预测结果\n    return predictions, loss, train_op\n": "Def lstm_model (X, y, Mode): # uses the multilayer LSTM structure cell = tf.nn.rnn_cell.MultiRNNCell ([tf.nn.rnn_cell.LSTMCell (HIDDEN_SIZE) for _ in range (NUM_LAYERS)] # to enable Using TensorFlow interface to connect multi-layer LSTM structure into RNN network and calculate its forward propagation result outputs, _ = tf.nn.dynamic_rnn (cell, X, dtype=tf.float32) output = outputs [:, -1,:] # adds a full link layer to the output of the LSTM network and calculates the loss, Note that the default loss here is the average # square difference loss function predictions = tf.contrib.layers.fully_connected (output, 1, activation_fn=None) if mode = tf.estimator.ModeKeys.PREDICT: return predictions, None,) None # calculated loss function loss = tf.losses.mean_squared_error (labels=y, predictions=predictions) if mode = tf.estimator.ModeKeys.EVAL: return predictions, loss,) None # creates the model optimizer and gets the tuning step train_op = tf.contrib.layers.optimize_loss (loss, tf.train.get_global_step (), optimizer= \"Adagrad\"), Learning_rate=0.1) # the loss function and optimization steps are calculated only during training, and the predicted result return predictions, loss, train_op is returned directly during test",
    "    def lstm_cell(cell_size, keep_prob, num_proj):\n        return tf.nn.rnn_cell.DropoutWrapper(\n            tf.nn.rnn_cell.LSTMCell(cell_size, num_proj=min(cell_size, num_proj)),\n            output_keep_prob=keep_prob)\n\n    def multi_lstm_cell(cell_sizes, keep_prob, num_proj):\n        return tf.nn.rnn_cell.MultiRNNCell([lstm_cell(cell_size, keep_prob, num_proj)\n                                            for cell_size in cell_sizes])\n\n    '''以前面定义的LSTM cell为基础定义多层堆叠的LSTM，我们这里只有1层'''\n    '''将已经堆叠起的LSTM单元转化成动态的可在训练过程中更新的LSTM单元'''\n    cell = multi_lstm_cell(cell_sizes=64, keep_prob=0.6, num_proj=64)": "Def lstm_cell (cell_size, keep_prob, num_proj): return tf.nn.rnn_cell.DropoutWrapper (tf.nn.rnn_cell.LSTMCell (cell_size, num_proj=min) (cell_size, num_proj),) Output_keep_prob=keep_prob) def multi_lstm_cell (cell_sizes, keep_prob, num_proj): return tf.nn.rnn_cell.MultiRNNCell ([lstm_cell (cell_size, keep_prob,) (num_proj) for cell_size in cell_sizes]) A multilayer stack of LSTM, based on the previously defined LSTM cell We have only one 'layer' here to convert the stacked LSTM units into dynamic LSTM units that can be updated during training 'cell = multi_lstm_cell (cell_sizes=64, keep_prob=0.6, num_proj=64)",
    "\n\ndef translate(textData):\n    try:\n        results = textData.replace('\\n', ' ').replace('- ', '')\n        results = re.sub(' +', ' ', results)\n        if results in translated:\n            return results, translated[textData]\n        res_tran = requests.post('http://fy.iciba.com/ajax.php?a=fy', data={'w': results}).json()\n        content = res_tran.get('content', {})\n        if 'out' in content.keys():\n            tran_txt = content['out']\n        elif 'word_mean' in content.keys():\n            tran_txt = '\\n'.join(content['word_mean'])\n        else:\n            return results, ''\n        # keep faster\n        if len(translated.keys()) > 100000:\n            os.rename('translated.json', 'translated.json.bak')\n            translated.clear()\n        translated[textData] = tran_txt\n        json.dump(translated, open('translated.json', 'w'), ensure_ascii=False, indent=4)\n        return results, tran_txt\n    except Exception as e:\n        return '', str(e)": "def转换(TextData)：如果结果被翻译为：返回结果，则尝试：结果=textData.替换(‘\\n’，‘).替换(’，‘)结果=re.sub(’，结果)，翻译：[textData]res_tran=requests.post(‘http：/fy.iciba.com/ajax.php？a=fy’，data={‘w’：Resules}).json()content=res_tran.get(‘content’，{})如果content.key()中的“out”：tran_txt=content[‘out’]Elif‘word_均’在content.key()中为‘tran_txt=’\\n‘.join(content[’word_meal‘])，否则：返回结果，‘#如果len(translated.key()>100000：os.rename(’translated.json‘，’translated.json.bak‘)translated.CLEAR()翻译为[textData]=tran_txt json.dump(翻译，打开(’translated.json‘，’w‘)，请确保_ascii=false，INDENT=4)返回结果，Tran_txt，除了异常e：back‘，str(E)",
    "keyPressEvent": "empressement真诚，热心",
    "# -*- coding: utf-8 -*-\nfrom PyQt5 import QtCore, QtGui, QtWidgets\n\nclass MyLineEdit(QtWidgets.QLineEdit):\n    def __init__(self, parent=None):\n        QtWidgets.QLineEdit.__init__(self, parent)\n        self.id = None\n    def event(self, e):\n        if e.type() == QtCore.QEvent.Shortcut:\n            if self.id == e.shortcutId():\n                self.setFocus(QtCore.Qt.ShortcutFocusReason)\n                return True\n        return QtWidgets.QLineEdit.event(self, e)\n\nclass MyWindow(QtWidgets.QWidget):\n    def __init__(self, parent=None):\n        QtWidgets.QWidget.__init__(self, parent)\n        self.resize(300, 100)\n        self.label = QtWidgets.QLabel(\"(&E)设置输入焦点在编辑框1\")\n        self.lineEdit1 = QtWidgets.QLineEdit()\n        self.label.setBuddy(self.lineEdit1)\n        self.lineEdit2 = MyLineEdit()\n        self.lineEdit2.id = self.lineEdit2.grabShortcut(\n                            QtGui.QKeySequence.mnemonic(\"&D\"))\n        self.button = QtWidgets.QPushButton(\"(&R)删除编辑框1的输入焦点\")\n        self.vbox = QtWidgets.QVBoxLayout()\n        self.vbox.addWidget(self.label)\n        self.vbox.addWidget(self.lineEdit1)\n        self.vbox.addWidget(self.lineEdit2)\n        self.vbox.addWidget(self.button)\n        self.setLayout(self.vbox)\n        self.button.clicked.connect(self.on_clicked)\n    def on_clicked(self):\n        self.lineEdit1.clearFocus()\n\nif __name__ == \"__main__\":\n    import sys\n    app = QtWidgets.QApplication(sys.argv)\n    window = MyWindow()\n    window.show()\n    sys.exit(app.exec_())": "#-* coding: utf-8-* from PyQt5 import QtCore, QtGui, QtWidgets class MyLineEdit (QtWidgets.QLineEdit): def _ init__ (self, parent=None): QtWidgets.QLineEdit.__init__ (self, parent) self.id = None def event (self, E): if e.type () = QtCore.QEvent.Shortcut: if self.id = e.shortcutId (): self.setFocus (QtCore.Qt.ShortcutFocusReason) return True return QtWidgets.QLineEdit.event (self, e) class MyWindow (QtWidgets.QWidget): def _ init__ (self,) Parent=None): QtWidgets.QWidget.__init__ (self, parent) self.resize (300, Self.label = QtWidgets.QLabel (\"(& E) input focus in edit box 1\") self.lineEdit1 = QtWidgets.QLineEdit () self.label.setBuddy (self.lineEdit1) self.lineEdit2 = MyLineEdit () self.lineEdit2.id = self.lineEdit2.grabShortcut (QtGui.QKeySequence.) Mnemonic (\"& D\") self.button = QtWidgets.QPushButton (\"(& R) Delete input focus of edit box 1\") self.vbox = QtWidgets.QVBoxLayout () self.vbox.addWidget (self.label) self.vbox.addWidget (self.lineEdit1) self.vbox.addWidget (self) LineEdit2) self.vbox.addWidget (self.button) self.setLayout (self.vbox) self.button.clicked.connect (self.on_clicked) def on_clicked (self): self.lineEdit1.clearFocus () if _ name__ = \"_ main__\": import sys app = QtWidgets . QApplication (sys.argv) window = MyWindow () window.show () sys.exit (app.exec_ ()",
    "MyLineEdit": "MyLineEdit",
    "class MyLineEdit(QtWidgets.QLineEdit):\n    def __init__(self, parent=None):\n        QtWidgets.QLineEdit.__init__(self, parent)\n        self.id = None\n    def event(self, e):\n        if e.type() == QtCore.QEvent.Shortcut:\n            if self.id == e.shortcutId():\n                self.setFocus(QtCore.Qt.ShortcutFocusReason)\n                return True\n        return QtWidgets.QLineEdit.event(self, e)": "类MyLineEdit(QtWidgets.QLineEdit)：def_init_(Self，Parent=None)：QtWidgets.QLineEdit._init_(Self，Parent)Sel.id=None def事件(Self，事件(Self，e)：如果e.type()=QtCore.QEvent.快捷方式：如果自我.id=.快捷方式()：Sel.setFocus(QtCore.Qt.Sht券FocusReason)返回真正的返回QtWidgets.QLineEdit.Event(Self，e)",
    "MyQLineEdit": "MyQLineEdit",
    "# 检测键盘回车按键，函数名字不要改，这是重写键盘事件\n    def keyPressEvent(self, event):\n        # 当需要组合键时，要很多种方式，这里举例为“shift+单个按键”，也可以采用shortcut、或者pressSequence的方法。\n        if event.key() == Qt.Key_Enter and QApplication.keyboardModifiers() == Qt.ShiftModifier:\n            print('key pressed!')\n            self.trans()": "# check keyboard enter keys, function name do not change, this is rewriting keyboard events def keyPressEvent (self, event): # when the need to combine keys, there are many ways, this example is \"shift single key\", or you can use shortcut, Or pressSequence's method. If event.key () = Qt.Key_Enter and QApplication.keyboardModifiers () = Qt.ShiftModifier: print ('key expressed') Self.trans ()",
    "# 检测键盘回车按键，函数名字不要改，这是重写键盘事件\n        def keyPressEvent(self, event):\n            # 当需要组合键时，要很多种方式，这里举例为“shift+单个按键”，也可以采用shortcut、或者pressSequence的方法。\n            if event.key() == Qt.Key_Enter and QApplication.keyboardModifiers() == Qt.ShiftModifier:\n                print('key pressed!')\n                self.trans()": "# check keyboard enter keys, function name do not change, this is rewriting keyboard events def keyPressEvent (self, event): # when the need to combine keys, there are many ways, this example is \"shift single key\", or you can use shortcut, Or pressSequence's method. If event.key () = Qt.Key_Enter and QApplication.keyboardModifiers () = Qt.ShiftModifier: print ('key expressed') Self.trans ()",
    "grabShortcut": "grabShortcut",
    "self.lineEdit2.id = ": "Sel.lineEdit2.id=",
    "具体什么意思？": "What do you mean?",
    "是月初的那个文档吗？": "Is that the document from the beginning of the month?",
    "(&E)": "(&E)",
    "(&T)": "(&T)",
    "QtWidgets": "widgets小器具，装饰品，窗口小部件( widget的名词复数 )",
    "": ""
}